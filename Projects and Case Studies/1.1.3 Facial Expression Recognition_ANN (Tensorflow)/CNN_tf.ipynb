{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_tf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3XN02gdtnb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/Lazy courses/DS:Deep Learning in Python/facial expression recognition')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxs7GSrHuYxa",
        "colab_type": "code",
        "outputId": "f4c1f1fe-055b-4b28-c4b6-dbf2d4d4ac2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from util import getImagedata, error_rate, init_weight_and_bias, y2indicator\n",
        "from ANN_tf import HiddenLayer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnStdO-HuYz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_filter(shape, poolsz):\n",
        "    shape = np.array(shape)\n",
        "    poolsz = np.array(poolsz)\n",
        "    #w = np.random.randn(*shape) * np.sqrt(2) / np.sqrt(np.prod(shape[:-1]) + shape[:-1] * np.prod((shape[:-2]/np.prod(poolsz))))\n",
        "    w = np.random.randn(*shape) * np.sqrt(2) / np.sqrt(np.prod(shape[:-1]) + shape[-1]*np.prod(shape[:-2] / np.prod(poolsz)))\n",
        "    return w.astype(np.float32)\n",
        "\n",
        "class ConvPoolLayer(object):\n",
        "    def __init__(self, mi, mo, fw=5, fh=5, poolsz = (2, 2)):\n",
        "        #mi = input feature map size\n",
        "        #mo = output feature map size\n",
        "        sz = (fw, fh, mi, mo)\n",
        "        W0 = init_filter(sz, poolsz)\n",
        "        self.W = tf.Variable(W0)\n",
        "        b0 = np.zeros(mo, dtype=np.float32)\n",
        "        self.b  = tf.Variable(b0)\n",
        "        self.poolsz = poolsz\n",
        "        self.params = [self.W, self.b]\n",
        "\n",
        "    def forward(self, X):\n",
        "        conv_out = tf.nn.conv2d(X, self.W, strides=[1,1,1,1], padding='SAME')\n",
        "        conv_out = tf.nn.bias_add(conv_out, self.b)\n",
        "        p1, p2 = self.poolsz\n",
        "        pool_out = tf.nn.max_pool(\n",
        "            conv_out,\n",
        "            ksize = [1, p1, p2, 1],\n",
        "            strides = [1, p1, p2, 1],\n",
        "            padding = 'SAME'\n",
        "        )\n",
        "\n",
        "        return tf.nn.relu(pool_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVjxc0C9uY35",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fc2455a0-1573-4914-8ca6-5ba2f164e015"
      },
      "source": [
        "class CNN(object):\n",
        "    def __init__(self, convpool_layer_sizes, hidden_layer_sizes):\n",
        "        self.convpool_layer_sizes = convpool_layer_sizes\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "\n",
        "    def fit(self, X, Y, Xvalid, Yvalid, lr=1e-2, mu=0.9, reg=1e-3, decay=0.99999, eps=1e-10, batch_sz=30,  epochs=5, show_fig=True):\n",
        "        lr = np.float32(lr)\n",
        "        mu = np.float32(mu)\n",
        "        reg = np.float32(reg)\n",
        "        decay = np.float32(decay)\n",
        "        eps = np.float32(eps)\n",
        "        K = len(set(Y))\n",
        "\n",
        "        # prepare a validation set\n",
        "        X, Y = shuffle(X, Y)\n",
        "        X = X.astype(np.float32)\n",
        "        Y = y2indicator(Y).astype(np.float32)\n",
        "\n",
        "        Yvalid = y2indicator(Yvalid).astype(np.float32)\n",
        "        Yvalid_flat = np.argmax(Yvalid, axis=1) # for calculating error rate\n",
        "\n",
        "        # initialize convpool layers\n",
        "        N, width, height, c = X.shape\n",
        "        mi = c\n",
        "        outw = width\n",
        "        outh = height\n",
        "        self.convpool_layers = []\n",
        "        for mo, fw, fh in self.convpool_layer_sizes:\n",
        "            layer = ConvPoolLayer(mi, mo, fw, fh)\n",
        "            self.convpool_layers.append(layer)\n",
        "            outw = outw // 2\n",
        "            outh = outh // 2\n",
        "            mi = mo\n",
        "\n",
        "        # initialize mlp layers\n",
        "        self.hidden_layers = []\n",
        "        M1 = self.convpool_layer_sizes[-1][0]*outw*outh #size must be same as output of last convpool layer\n",
        "        count = 0\n",
        "        for M2 in self.hidden_layer_sizes:\n",
        "            h = HiddenLayer(M1, M2, count)\n",
        "            self.hidden_layers.append(h)  \n",
        "            M1 = M2\n",
        "            count += 1\n",
        "\n",
        "         #logistic regression layer\n",
        "        W, b = init_weight_and_bias(M1, K)\n",
        "        self.W = tf.Variable(W, 'W_logreg')\n",
        "        self.b = tf.Variable(b, 'b_logreg')\n",
        "\n",
        "        # collect params for later use        \n",
        "        self.params = [self.W, self.b]\n",
        "        for h in self.convpool_layers:\n",
        "            self.params += h.params\n",
        "        for h in self.hidden_layers:\n",
        "            self.params += h.params\n",
        "\n",
        "        #set up tensorflow functions and variables\n",
        "        tfX = tf.placeholder(tf.float32, shape=(None, width, height, c), name='X')\n",
        "        tfY = tf.placeholder(tf.float32, shape= (None, K), name='Y')\n",
        "        act = self.forward(tfX)\n",
        "\n",
        "        rcost = reg*sum([tf.nn.l2_loss(p) for p in self.params])\n",
        "        cost = tf.reduce_mean(\n",
        "            tf.nn.softmax_cross_entropy_with_logits(\n",
        "                logits=act,\n",
        "                labels=tfY\n",
        "            )\n",
        "        ) + rcost\n",
        "\n",
        "        prediction = self.predict(tfX)\n",
        "\n",
        "        train_op = tf.train.RMSPropOptimizer(lr, decay=decay, momentum=mu).minimize(cost)\n",
        "\n",
        "        n_batches = N // batch_sz\n",
        "        costs = []\n",
        "        init = tf.global_variables_initializer()\n",
        "        with tf.Session() as session:\n",
        "            session.run(init)\n",
        "            for i in range(epochs):\n",
        "                X, Y  = shuffle(X, Y)\n",
        "                for j in range(n_batches):\n",
        "                    Xbatch = X[j*batch_sz:(j*batch_sz+batch_sz)]\n",
        "                    Ybatch = Y[j*batch_sz:(j*batch_sz+batch_sz)]\n",
        "\n",
        "                    session.run(train_op, feed_dict={tfX: Xbatch, tfY: Ybatch})\n",
        "\n",
        "                    if j % 20 == 0:\n",
        "                        c = session.run(cost, feed_dict={tfX: Xvalid, tfY: Yvalid})\n",
        "                        costs.append(c)\n",
        "\n",
        "                        p = session.run(prediction, feed_dict={tfX:Xvalid, tfY:Yvalid})\n",
        "                        costs.append(c)\n",
        "\n",
        "                        p = session.run(prediction, feed_dict= {tfX:Xvalid, tfY:Yvalid})\n",
        "                        e = error_rate(Yvalid_flat, p)\n",
        "                        print(f\"i: {i} j: {j}, nb: {n_batches}, cost: {c}, error rate: {e}\")\n",
        "\n",
        "        if show_fig:\n",
        "            plt.plot(costs)\n",
        "            plt.show()\n",
        "\n",
        "    def forward(self, X):\n",
        "        Z = X\n",
        "        for c in self.convpool_layers:\n",
        "            Z = c.forward(Z)\n",
        "        Z_shape = Z.get_shape().as_list()\n",
        "        Z = tf.reshape(Z, [-1, np.prod(Z_shape[1:])])\n",
        "        for h in self.hidden_layers:\n",
        "            Z = h.forward(Z)\n",
        "        return tf.matmul(Z, self.W) + self.b\n",
        "\n",
        "    def predict(self, X):\n",
        "        pY = self.forward(X)\n",
        "        return tf.argmax(pY, 1)\n",
        "\n",
        "\n",
        "def main():\n",
        "    Xtrain, Ytrain, Xvalid, Yvalid = getImagedata()\n",
        "\n",
        "    #reshape X for tf: N x H x W x C\n",
        "    Xtrain = Xtrain.transpose((0, 2, 3, 1))\n",
        "    Xvalid = Xvalid.transpose((0, 2, 3, 1))\n",
        "\n",
        "    model = CNN(\n",
        "        convpool_layer_sizes=[(20, 5, 5), (20, 5, 5)],\n",
        "        hidden_layer_sizes = [500, 300]\n",
        "    )\n",
        "    model.fit(Xtrain, Ytrain, Xvalid, Yvalid)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-cc0472ae2c9b>:66: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "i: 0 j: 0, nb: 1303, cost: 2.3503997325897217, error rate: 0.813\n",
            "i: 0 j: 20, nb: 1303, cost: 2.3385887145996094, error rate: 0.74\n",
            "i: 0 j: 40, nb: 1303, cost: 2.330146551132202, error rate: 0.74\n",
            "i: 0 j: 60, nb: 1303, cost: 2.29823899269104, error rate: 0.74\n",
            "i: 0 j: 80, nb: 1303, cost: 2.325718402862549, error rate: 0.74\n",
            "i: 0 j: 100, nb: 1303, cost: 2.3159570693969727, error rate: 0.74\n",
            "i: 0 j: 120, nb: 1303, cost: 2.3100569248199463, error rate: 0.74\n",
            "i: 0 j: 140, nb: 1303, cost: 2.3141794204711914, error rate: 0.74\n",
            "i: 0 j: 160, nb: 1303, cost: 2.288508176803589, error rate: 0.733\n",
            "i: 0 j: 180, nb: 1303, cost: 2.260951280593872, error rate: 0.74\n",
            "i: 0 j: 200, nb: 1303, cost: 2.2622129917144775, error rate: 0.74\n",
            "i: 0 j: 220, nb: 1303, cost: 2.2618091106414795, error rate: 0.738\n",
            "i: 0 j: 240, nb: 1303, cost: 2.261213779449463, error rate: 0.731\n",
            "i: 0 j: 260, nb: 1303, cost: 2.286919116973877, error rate: 0.737\n",
            "i: 0 j: 280, nb: 1303, cost: 2.247683048248291, error rate: 0.725\n",
            "i: 0 j: 300, nb: 1303, cost: 2.2341649532318115, error rate: 0.688\n",
            "i: 0 j: 320, nb: 1303, cost: 2.228424072265625, error rate: 0.713\n",
            "i: 0 j: 340, nb: 1303, cost: 2.2040517330169678, error rate: 0.688\n",
            "i: 0 j: 360, nb: 1303, cost: 2.1726293563842773, error rate: 0.683\n",
            "i: 0 j: 380, nb: 1303, cost: 2.1436727046966553, error rate: 0.654\n",
            "i: 0 j: 400, nb: 1303, cost: 2.1371288299560547, error rate: 0.646\n",
            "i: 0 j: 420, nb: 1303, cost: 2.137967586517334, error rate: 0.686\n",
            "i: 0 j: 440, nb: 1303, cost: 2.221593141555786, error rate: 0.753\n",
            "i: 0 j: 460, nb: 1303, cost: 2.128230571746826, error rate: 0.661\n",
            "i: 0 j: 480, nb: 1303, cost: 2.2850241661071777, error rate: 0.792\n",
            "i: 0 j: 500, nb: 1303, cost: 2.1091482639312744, error rate: 0.676\n",
            "i: 0 j: 520, nb: 1303, cost: 2.1159603595733643, error rate: 0.673\n",
            "i: 0 j: 540, nb: 1303, cost: 2.1092700958251953, error rate: 0.665\n",
            "i: 0 j: 560, nb: 1303, cost: 2.064227819442749, error rate: 0.642\n",
            "i: 0 j: 580, nb: 1303, cost: 2.0843818187713623, error rate: 0.675\n",
            "i: 0 j: 600, nb: 1303, cost: 2.0515730381011963, error rate: 0.629\n",
            "i: 0 j: 620, nb: 1303, cost: 2.010183334350586, error rate: 0.616\n",
            "i: 0 j: 640, nb: 1303, cost: 2.00805401802063, error rate: 0.622\n",
            "i: 0 j: 660, nb: 1303, cost: 2.0903568267822266, error rate: 0.675\n",
            "i: 0 j: 680, nb: 1303, cost: 2.0145444869995117, error rate: 0.639\n",
            "i: 0 j: 700, nb: 1303, cost: 2.071558952331543, error rate: 0.67\n",
            "i: 0 j: 720, nb: 1303, cost: 2.03810977935791, error rate: 0.657\n",
            "i: 0 j: 740, nb: 1303, cost: 1.9874234199523926, error rate: 0.636\n",
            "i: 0 j: 760, nb: 1303, cost: 2.026444435119629, error rate: 0.656\n",
            "i: 0 j: 780, nb: 1303, cost: 1.999382495880127, error rate: 0.637\n",
            "i: 0 j: 800, nb: 1303, cost: 1.9836421012878418, error rate: 0.621\n",
            "i: 0 j: 820, nb: 1303, cost: 1.9911503791809082, error rate: 0.628\n",
            "i: 0 j: 840, nb: 1303, cost: 1.9767276048660278, error rate: 0.647\n",
            "i: 0 j: 860, nb: 1303, cost: 1.9614837169647217, error rate: 0.622\n",
            "i: 0 j: 880, nb: 1303, cost: 2.002103805541992, error rate: 0.646\n",
            "i: 0 j: 900, nb: 1303, cost: 1.9596660137176514, error rate: 0.625\n",
            "i: 0 j: 920, nb: 1303, cost: 1.9341859817504883, error rate: 0.618\n",
            "i: 0 j: 940, nb: 1303, cost: 1.9803707599639893, error rate: 0.628\n",
            "i: 0 j: 960, nb: 1303, cost: 1.916590929031372, error rate: 0.612\n",
            "i: 0 j: 980, nb: 1303, cost: 2.0492849349975586, error rate: 0.664\n",
            "i: 0 j: 1000, nb: 1303, cost: 1.8947292566299438, error rate: 0.597\n",
            "i: 0 j: 1020, nb: 1303, cost: 1.9504374265670776, error rate: 0.648\n",
            "i: 0 j: 1040, nb: 1303, cost: 1.9633405208587646, error rate: 0.627\n",
            "i: 0 j: 1060, nb: 1303, cost: 1.9113740921020508, error rate: 0.605\n",
            "i: 0 j: 1080, nb: 1303, cost: 1.912805199623108, error rate: 0.609\n",
            "i: 0 j: 1100, nb: 1303, cost: 1.9509894847869873, error rate: 0.627\n",
            "i: 0 j: 1120, nb: 1303, cost: 1.9268378019332886, error rate: 0.611\n",
            "i: 0 j: 1140, nb: 1303, cost: 1.888003945350647, error rate: 0.595\n",
            "i: 0 j: 1160, nb: 1303, cost: 1.9037703275680542, error rate: 0.623\n",
            "i: 0 j: 1180, nb: 1303, cost: 1.8882153034210205, error rate: 0.606\n",
            "i: 0 j: 1200, nb: 1303, cost: 1.9232957363128662, error rate: 0.619\n",
            "i: 0 j: 1220, nb: 1303, cost: 1.9389609098434448, error rate: 0.635\n",
            "i: 0 j: 1240, nb: 1303, cost: 1.8902488946914673, error rate: 0.601\n",
            "i: 0 j: 1260, nb: 1303, cost: 1.855900764465332, error rate: 0.594\n",
            "i: 0 j: 1280, nb: 1303, cost: 1.8645764589309692, error rate: 0.609\n",
            "i: 0 j: 1300, nb: 1303, cost: 1.8885496854782104, error rate: 0.604\n",
            "i: 1 j: 0, nb: 1303, cost: 1.8720539808273315, error rate: 0.599\n",
            "i: 1 j: 20, nb: 1303, cost: 1.944783091545105, error rate: 0.616\n",
            "i: 1 j: 40, nb: 1303, cost: 1.8578169345855713, error rate: 0.584\n",
            "i: 1 j: 60, nb: 1303, cost: 1.8574533462524414, error rate: 0.594\n",
            "i: 1 j: 80, nb: 1303, cost: 1.8504482507705688, error rate: 0.587\n",
            "i: 1 j: 100, nb: 1303, cost: 1.8429491519927979, error rate: 0.59\n",
            "i: 1 j: 120, nb: 1303, cost: 1.8215960264205933, error rate: 0.579\n",
            "i: 1 j: 140, nb: 1303, cost: 1.931976079940796, error rate: 0.633\n",
            "i: 1 j: 160, nb: 1303, cost: 1.8765344619750977, error rate: 0.606\n",
            "i: 1 j: 180, nb: 1303, cost: 1.861393928527832, error rate: 0.598\n",
            "i: 1 j: 200, nb: 1303, cost: 1.820815086364746, error rate: 0.574\n",
            "i: 1 j: 220, nb: 1303, cost: 1.8700460195541382, error rate: 0.594\n",
            "i: 1 j: 240, nb: 1303, cost: 1.8050841093063354, error rate: 0.558\n",
            "i: 1 j: 260, nb: 1303, cost: 1.838892936706543, error rate: 0.592\n",
            "i: 1 j: 280, nb: 1303, cost: 2.016221046447754, error rate: 0.646\n",
            "i: 1 j: 300, nb: 1303, cost: 1.8740284442901611, error rate: 0.607\n",
            "i: 1 j: 320, nb: 1303, cost: 1.895244836807251, error rate: 0.609\n",
            "i: 1 j: 340, nb: 1303, cost: 1.8452050685882568, error rate: 0.606\n",
            "i: 1 j: 360, nb: 1303, cost: 1.8215503692626953, error rate: 0.594\n",
            "i: 1 j: 380, nb: 1303, cost: 1.814698576927185, error rate: 0.575\n",
            "i: 1 j: 400, nb: 1303, cost: 1.8628841638565063, error rate: 0.608\n",
            "i: 1 j: 420, nb: 1303, cost: 1.8124665021896362, error rate: 0.571\n",
            "i: 1 j: 440, nb: 1303, cost: 1.8397741317749023, error rate: 0.582\n",
            "i: 1 j: 460, nb: 1303, cost: 1.8089975118637085, error rate: 0.585\n",
            "i: 1 j: 480, nb: 1303, cost: 1.7909350395202637, error rate: 0.592\n",
            "i: 1 j: 500, nb: 1303, cost: 1.8343106508255005, error rate: 0.582\n",
            "i: 1 j: 520, nb: 1303, cost: 1.8189517259597778, error rate: 0.576\n",
            "i: 1 j: 540, nb: 1303, cost: 1.821850299835205, error rate: 0.583\n",
            "i: 1 j: 560, nb: 1303, cost: 1.794573426246643, error rate: 0.572\n",
            "i: 1 j: 580, nb: 1303, cost: 1.8170826435089111, error rate: 0.578\n",
            "i: 1 j: 600, nb: 1303, cost: 1.8108391761779785, error rate: 0.57\n",
            "i: 1 j: 620, nb: 1303, cost: 1.8099591732025146, error rate: 0.588\n",
            "i: 1 j: 640, nb: 1303, cost: 1.826401948928833, error rate: 0.576\n",
            "i: 1 j: 660, nb: 1303, cost: 1.8768986463546753, error rate: 0.592\n",
            "i: 1 j: 680, nb: 1303, cost: 1.8533071279525757, error rate: 0.594\n",
            "i: 1 j: 700, nb: 1303, cost: 1.8623172044754028, error rate: 0.587\n",
            "i: 1 j: 720, nb: 1303, cost: 1.787824273109436, error rate: 0.566\n",
            "i: 1 j: 740, nb: 1303, cost: 1.8001682758331299, error rate: 0.576\n",
            "i: 1 j: 760, nb: 1303, cost: 1.7727382183074951, error rate: 0.56\n",
            "i: 1 j: 780, nb: 1303, cost: 1.7463018894195557, error rate: 0.563\n",
            "i: 1 j: 800, nb: 1303, cost: 1.8046362400054932, error rate: 0.569\n",
            "i: 1 j: 820, nb: 1303, cost: 1.7652881145477295, error rate: 0.565\n",
            "i: 1 j: 840, nb: 1303, cost: 1.7398655414581299, error rate: 0.546\n",
            "i: 1 j: 860, nb: 1303, cost: 1.7810423374176025, error rate: 0.575\n",
            "i: 1 j: 880, nb: 1303, cost: 1.7497814893722534, error rate: 0.56\n",
            "i: 1 j: 900, nb: 1303, cost: 1.7784838676452637, error rate: 0.551\n",
            "i: 1 j: 920, nb: 1303, cost: 1.7753124237060547, error rate: 0.574\n",
            "i: 1 j: 940, nb: 1303, cost: 1.7733020782470703, error rate: 0.571\n",
            "i: 1 j: 960, nb: 1303, cost: 1.7544598579406738, error rate: 0.564\n",
            "i: 1 j: 980, nb: 1303, cost: 1.7651851177215576, error rate: 0.562\n",
            "i: 1 j: 1000, nb: 1303, cost: 1.7944751977920532, error rate: 0.578\n",
            "i: 1 j: 1020, nb: 1303, cost: 1.7802340984344482, error rate: 0.563\n",
            "i: 1 j: 1040, nb: 1303, cost: 1.7370191812515259, error rate: 0.55\n",
            "i: 1 j: 1060, nb: 1303, cost: 1.7912391424179077, error rate: 0.569\n",
            "i: 1 j: 1080, nb: 1303, cost: 1.7589101791381836, error rate: 0.57\n",
            "i: 1 j: 1100, nb: 1303, cost: 1.7899460792541504, error rate: 0.578\n",
            "i: 1 j: 1120, nb: 1303, cost: 1.7490184307098389, error rate: 0.558\n",
            "i: 1 j: 1140, nb: 1303, cost: 1.8170807361602783, error rate: 0.588\n",
            "i: 1 j: 1160, nb: 1303, cost: 1.713451862335205, error rate: 0.532\n",
            "i: 1 j: 1180, nb: 1303, cost: 1.7341184616088867, error rate: 0.545\n",
            "i: 1 j: 1200, nb: 1303, cost: 1.77168607711792, error rate: 0.569\n",
            "i: 1 j: 1220, nb: 1303, cost: 1.7351936101913452, error rate: 0.543\n",
            "i: 1 j: 1240, nb: 1303, cost: 1.717329978942871, error rate: 0.545\n",
            "i: 1 j: 1260, nb: 1303, cost: 1.7247909307479858, error rate: 0.545\n",
            "i: 1 j: 1280, nb: 1303, cost: 1.7011535167694092, error rate: 0.546\n",
            "i: 1 j: 1300, nb: 1303, cost: 1.7602927684783936, error rate: 0.562\n",
            "i: 2 j: 0, nb: 1303, cost: 1.7412540912628174, error rate: 0.556\n",
            "i: 2 j: 20, nb: 1303, cost: 1.7944939136505127, error rate: 0.581\n",
            "i: 2 j: 40, nb: 1303, cost: 1.8015393018722534, error rate: 0.572\n",
            "i: 2 j: 60, nb: 1303, cost: 1.7358819246292114, error rate: 0.551\n",
            "i: 2 j: 80, nb: 1303, cost: 1.6844829320907593, error rate: 0.525\n",
            "i: 2 j: 100, nb: 1303, cost: 1.7613320350646973, error rate: 0.554\n",
            "i: 2 j: 120, nb: 1303, cost: 1.7309539318084717, error rate: 0.55\n",
            "i: 2 j: 140, nb: 1303, cost: 1.6825934648513794, error rate: 0.525\n",
            "i: 2 j: 160, nb: 1303, cost: 1.722677230834961, error rate: 0.551\n",
            "i: 2 j: 180, nb: 1303, cost: 1.7259817123413086, error rate: 0.546\n",
            "i: 2 j: 200, nb: 1303, cost: 1.7371904850006104, error rate: 0.566\n",
            "i: 2 j: 220, nb: 1303, cost: 1.747602105140686, error rate: 0.567\n",
            "i: 2 j: 240, nb: 1303, cost: 1.7034865617752075, error rate: 0.542\n",
            "i: 2 j: 260, nb: 1303, cost: 1.7169933319091797, error rate: 0.525\n",
            "i: 2 j: 280, nb: 1303, cost: 1.76593816280365, error rate: 0.574\n",
            "i: 2 j: 300, nb: 1303, cost: 1.7327768802642822, error rate: 0.537\n",
            "i: 2 j: 320, nb: 1303, cost: 1.7381906509399414, error rate: 0.543\n",
            "i: 2 j: 340, nb: 1303, cost: 1.708924412727356, error rate: 0.528\n",
            "i: 2 j: 360, nb: 1303, cost: 1.7116467952728271, error rate: 0.53\n",
            "i: 2 j: 380, nb: 1303, cost: 1.7009773254394531, error rate: 0.55\n",
            "i: 2 j: 400, nb: 1303, cost: 1.7420165538787842, error rate: 0.55\n",
            "i: 2 j: 420, nb: 1303, cost: 1.7193388938903809, error rate: 0.555\n",
            "i: 2 j: 440, nb: 1303, cost: 1.7483364343643188, error rate: 0.564\n",
            "i: 2 j: 460, nb: 1303, cost: 1.735798954963684, error rate: 0.555\n",
            "i: 2 j: 480, nb: 1303, cost: 1.7287428379058838, error rate: 0.551\n",
            "i: 2 j: 500, nb: 1303, cost: 1.6852049827575684, error rate: 0.527\n",
            "i: 2 j: 520, nb: 1303, cost: 1.641961693763733, error rate: 0.512\n",
            "i: 2 j: 540, nb: 1303, cost: 1.6782923936843872, error rate: 0.521\n",
            "i: 2 j: 560, nb: 1303, cost: 1.7875736951828003, error rate: 0.556\n",
            "i: 2 j: 580, nb: 1303, cost: 1.698049783706665, error rate: 0.535\n",
            "i: 2 j: 600, nb: 1303, cost: 1.6968474388122559, error rate: 0.524\n",
            "i: 2 j: 620, nb: 1303, cost: 1.716273546218872, error rate: 0.54\n",
            "i: 2 j: 640, nb: 1303, cost: 1.6934123039245605, error rate: 0.538\n",
            "i: 2 j: 660, nb: 1303, cost: 1.6788835525512695, error rate: 0.515\n",
            "i: 2 j: 680, nb: 1303, cost: 1.7339863777160645, error rate: 0.546\n",
            "i: 2 j: 700, nb: 1303, cost: 1.7196428775787354, error rate: 0.548\n",
            "i: 2 j: 720, nb: 1303, cost: 1.6654891967773438, error rate: 0.527\n",
            "i: 2 j: 740, nb: 1303, cost: 1.6693997383117676, error rate: 0.541\n",
            "i: 2 j: 760, nb: 1303, cost: 1.6353633403778076, error rate: 0.513\n",
            "i: 2 j: 780, nb: 1303, cost: 1.6545811891555786, error rate: 0.52\n",
            "i: 2 j: 800, nb: 1303, cost: 1.6473113298416138, error rate: 0.524\n",
            "i: 2 j: 820, nb: 1303, cost: 1.6422715187072754, error rate: 0.515\n",
            "i: 2 j: 840, nb: 1303, cost: 1.7003334760665894, error rate: 0.545\n",
            "i: 2 j: 860, nb: 1303, cost: 1.6746461391448975, error rate: 0.529\n",
            "i: 2 j: 880, nb: 1303, cost: 1.6853476762771606, error rate: 0.534\n",
            "i: 2 j: 900, nb: 1303, cost: 1.663086175918579, error rate: 0.523\n",
            "i: 2 j: 920, nb: 1303, cost: 1.6555355787277222, error rate: 0.546\n",
            "i: 2 j: 940, nb: 1303, cost: 1.6454633474349976, error rate: 0.52\n",
            "i: 2 j: 960, nb: 1303, cost: 1.651865839958191, error rate: 0.515\n",
            "i: 2 j: 980, nb: 1303, cost: 1.6710929870605469, error rate: 0.525\n",
            "i: 2 j: 1000, nb: 1303, cost: 1.7106592655181885, error rate: 0.56\n",
            "i: 2 j: 1020, nb: 1303, cost: 1.6784846782684326, error rate: 0.543\n",
            "i: 2 j: 1040, nb: 1303, cost: 1.6459225416183472, error rate: 0.526\n",
            "i: 2 j: 1060, nb: 1303, cost: 1.7443357706069946, error rate: 0.548\n",
            "i: 2 j: 1080, nb: 1303, cost: 1.6864756345748901, error rate: 0.525\n",
            "i: 2 j: 1100, nb: 1303, cost: 1.639195203781128, error rate: 0.528\n",
            "i: 2 j: 1120, nb: 1303, cost: 1.6551040410995483, error rate: 0.54\n",
            "i: 2 j: 1140, nb: 1303, cost: 1.6673098802566528, error rate: 0.519\n",
            "i: 2 j: 1160, nb: 1303, cost: 1.6614428758621216, error rate: 0.522\n",
            "i: 2 j: 1180, nb: 1303, cost: 1.6947298049926758, error rate: 0.55\n",
            "i: 2 j: 1200, nb: 1303, cost: 1.6377170085906982, error rate: 0.515\n",
            "i: 2 j: 1220, nb: 1303, cost: 1.6703791618347168, error rate: 0.527\n",
            "i: 2 j: 1240, nb: 1303, cost: 1.6339523792266846, error rate: 0.518\n",
            "i: 2 j: 1260, nb: 1303, cost: 1.7099907398223877, error rate: 0.529\n",
            "i: 2 j: 1280, nb: 1303, cost: 1.7099804878234863, error rate: 0.544\n",
            "i: 2 j: 1300, nb: 1303, cost: 1.6482431888580322, error rate: 0.523\n",
            "i: 3 j: 0, nb: 1303, cost: 1.6605989933013916, error rate: 0.521\n",
            "i: 3 j: 20, nb: 1303, cost: 1.654129147529602, error rate: 0.513\n",
            "i: 3 j: 40, nb: 1303, cost: 1.6577143669128418, error rate: 0.525\n",
            "i: 3 j: 60, nb: 1303, cost: 1.6535180807113647, error rate: 0.519\n",
            "i: 3 j: 80, nb: 1303, cost: 1.6363383531570435, error rate: 0.526\n",
            "i: 3 j: 100, nb: 1303, cost: 1.6396963596343994, error rate: 0.516\n",
            "i: 3 j: 120, nb: 1303, cost: 1.6510977745056152, error rate: 0.535\n",
            "i: 3 j: 140, nb: 1303, cost: 1.6328966617584229, error rate: 0.51\n",
            "i: 3 j: 160, nb: 1303, cost: 1.6703413724899292, error rate: 0.519\n",
            "i: 3 j: 180, nb: 1303, cost: 1.714858055114746, error rate: 0.529\n",
            "i: 3 j: 200, nb: 1303, cost: 1.657662272453308, error rate: 0.495\n",
            "i: 3 j: 220, nb: 1303, cost: 1.6457786560058594, error rate: 0.521\n",
            "i: 3 j: 240, nb: 1303, cost: 1.6651756763458252, error rate: 0.519\n",
            "i: 3 j: 260, nb: 1303, cost: 1.6482934951782227, error rate: 0.513\n",
            "i: 3 j: 280, nb: 1303, cost: 1.627075433731079, error rate: 0.505\n",
            "i: 3 j: 300, nb: 1303, cost: 1.6376930475234985, error rate: 0.497\n",
            "i: 3 j: 320, nb: 1303, cost: 1.644255518913269, error rate: 0.505\n",
            "i: 3 j: 340, nb: 1303, cost: 1.6430723667144775, error rate: 0.529\n",
            "i: 3 j: 360, nb: 1303, cost: 1.6669363975524902, error rate: 0.536\n",
            "i: 3 j: 380, nb: 1303, cost: 1.6572073698043823, error rate: 0.528\n",
            "i: 3 j: 400, nb: 1303, cost: 1.66066575050354, error rate: 0.527\n",
            "i: 3 j: 420, nb: 1303, cost: 1.643876075744629, error rate: 0.508\n",
            "i: 3 j: 440, nb: 1303, cost: 1.707021951675415, error rate: 0.548\n",
            "i: 3 j: 460, nb: 1303, cost: 1.6062294244766235, error rate: 0.51\n",
            "i: 3 j: 480, nb: 1303, cost: 1.6004093885421753, error rate: 0.502\n",
            "i: 3 j: 500, nb: 1303, cost: 1.6184004545211792, error rate: 0.499\n",
            "i: 3 j: 520, nb: 1303, cost: 1.6044436693191528, error rate: 0.5\n",
            "i: 3 j: 540, nb: 1303, cost: 1.6187934875488281, error rate: 0.515\n",
            "i: 3 j: 560, nb: 1303, cost: 1.5976451635360718, error rate: 0.496\n",
            "i: 3 j: 580, nb: 1303, cost: 1.6049515008926392, error rate: 0.501\n",
            "i: 3 j: 600, nb: 1303, cost: 1.6019023656845093, error rate: 0.484\n",
            "i: 3 j: 620, nb: 1303, cost: 1.6086231470108032, error rate: 0.496\n",
            "i: 3 j: 640, nb: 1303, cost: 1.6141752004623413, error rate: 0.506\n",
            "i: 3 j: 660, nb: 1303, cost: 1.645750880241394, error rate: 0.497\n",
            "i: 3 j: 680, nb: 1303, cost: 1.7241365909576416, error rate: 0.564\n",
            "i: 3 j: 700, nb: 1303, cost: 1.632275104522705, error rate: 0.495\n",
            "i: 3 j: 720, nb: 1303, cost: 1.6071418523788452, error rate: 0.515\n",
            "i: 3 j: 740, nb: 1303, cost: 1.6199394464492798, error rate: 0.495\n",
            "i: 3 j: 760, nb: 1303, cost: 1.6321088075637817, error rate: 0.512\n",
            "i: 3 j: 780, nb: 1303, cost: 1.5913641452789307, error rate: 0.494\n",
            "i: 3 j: 800, nb: 1303, cost: 1.5665067434310913, error rate: 0.487\n",
            "i: 3 j: 820, nb: 1303, cost: 1.580230474472046, error rate: 0.478\n",
            "i: 3 j: 840, nb: 1303, cost: 1.6046839952468872, error rate: 0.506\n",
            "i: 3 j: 860, nb: 1303, cost: 1.6000200510025024, error rate: 0.496\n",
            "i: 3 j: 880, nb: 1303, cost: 1.5615061521530151, error rate: 0.491\n",
            "i: 3 j: 900, nb: 1303, cost: 1.6103122234344482, error rate: 0.512\n",
            "i: 3 j: 920, nb: 1303, cost: 1.589766263961792, error rate: 0.487\n",
            "i: 3 j: 940, nb: 1303, cost: 1.598644733428955, error rate: 0.513\n",
            "i: 3 j: 960, nb: 1303, cost: 1.5925666093826294, error rate: 0.485\n",
            "i: 3 j: 980, nb: 1303, cost: 1.5927035808563232, error rate: 0.502\n",
            "i: 3 j: 1000, nb: 1303, cost: 1.5814497470855713, error rate: 0.497\n",
            "i: 3 j: 1020, nb: 1303, cost: 1.5938951969146729, error rate: 0.488\n",
            "i: 3 j: 1040, nb: 1303, cost: 1.6392422914505005, error rate: 0.514\n",
            "i: 3 j: 1060, nb: 1303, cost: 1.5807452201843262, error rate: 0.496\n",
            "i: 3 j: 1080, nb: 1303, cost: 1.5979150533676147, error rate: 0.494\n",
            "i: 3 j: 1100, nb: 1303, cost: 1.6302340030670166, error rate: 0.503\n",
            "i: 3 j: 1120, nb: 1303, cost: 1.613620400428772, error rate: 0.515\n",
            "i: 3 j: 1140, nb: 1303, cost: 1.5938782691955566, error rate: 0.492\n",
            "i: 3 j: 1160, nb: 1303, cost: 1.6064136028289795, error rate: 0.517\n",
            "i: 3 j: 1180, nb: 1303, cost: 1.5542904138565063, error rate: 0.479\n",
            "i: 3 j: 1200, nb: 1303, cost: 1.5757139921188354, error rate: 0.479\n",
            "i: 3 j: 1220, nb: 1303, cost: 1.5942294597625732, error rate: 0.495\n",
            "i: 3 j: 1240, nb: 1303, cost: 1.537545919418335, error rate: 0.463\n",
            "i: 3 j: 1260, nb: 1303, cost: 1.5751047134399414, error rate: 0.487\n",
            "i: 3 j: 1280, nb: 1303, cost: 1.6048725843429565, error rate: 0.507\n",
            "i: 3 j: 1300, nb: 1303, cost: 1.6281839609146118, error rate: 0.505\n",
            "i: 4 j: 0, nb: 1303, cost: 1.6400355100631714, error rate: 0.514\n",
            "i: 4 j: 20, nb: 1303, cost: 1.621167778968811, error rate: 0.511\n",
            "i: 4 j: 40, nb: 1303, cost: 1.6139976978302002, error rate: 0.489\n",
            "i: 4 j: 60, nb: 1303, cost: 1.613152265548706, error rate: 0.505\n",
            "i: 4 j: 80, nb: 1303, cost: 1.5875632762908936, error rate: 0.492\n",
            "i: 4 j: 100, nb: 1303, cost: 1.563693642616272, error rate: 0.483\n",
            "i: 4 j: 120, nb: 1303, cost: 1.641327142715454, error rate: 0.496\n",
            "i: 4 j: 140, nb: 1303, cost: 1.5999006032943726, error rate: 0.486\n",
            "i: 4 j: 160, nb: 1303, cost: 1.6450029611587524, error rate: 0.521\n",
            "i: 4 j: 180, nb: 1303, cost: 1.575615406036377, error rate: 0.486\n",
            "i: 4 j: 200, nb: 1303, cost: 1.6391115188598633, error rate: 0.498\n",
            "i: 4 j: 220, nb: 1303, cost: 1.5960607528686523, error rate: 0.499\n",
            "i: 4 j: 240, nb: 1303, cost: 1.5856379270553589, error rate: 0.471\n",
            "i: 4 j: 260, nb: 1303, cost: 1.614439606666565, error rate: 0.502\n",
            "i: 4 j: 280, nb: 1303, cost: 1.6010748147964478, error rate: 0.491\n",
            "i: 4 j: 300, nb: 1303, cost: 1.5915050506591797, error rate: 0.509\n",
            "i: 4 j: 320, nb: 1303, cost: 1.6449155807495117, error rate: 0.497\n",
            "i: 4 j: 340, nb: 1303, cost: 1.5895739793777466, error rate: 0.483\n",
            "i: 4 j: 360, nb: 1303, cost: 1.6118762493133545, error rate: 0.496\n",
            "i: 4 j: 380, nb: 1303, cost: 1.6464003324508667, error rate: 0.503\n",
            "i: 4 j: 400, nb: 1303, cost: 1.6425305604934692, error rate: 0.505\n",
            "i: 4 j: 420, nb: 1303, cost: 1.6014317274093628, error rate: 0.486\n",
            "i: 4 j: 440, nb: 1303, cost: 1.618160367012024, error rate: 0.513\n",
            "i: 4 j: 460, nb: 1303, cost: 1.6226568222045898, error rate: 0.508\n",
            "i: 4 j: 480, nb: 1303, cost: 1.6394867897033691, error rate: 0.511\n",
            "i: 4 j: 500, nb: 1303, cost: 1.6175076961517334, error rate: 0.499\n",
            "i: 4 j: 520, nb: 1303, cost: 1.6188955307006836, error rate: 0.475\n",
            "i: 4 j: 540, nb: 1303, cost: 1.6806998252868652, error rate: 0.511\n",
            "i: 4 j: 560, nb: 1303, cost: 1.6612757444381714, error rate: 0.513\n",
            "i: 4 j: 580, nb: 1303, cost: 1.6206448078155518, error rate: 0.488\n",
            "i: 4 j: 600, nb: 1303, cost: 1.5898621082305908, error rate: 0.484\n",
            "i: 4 j: 620, nb: 1303, cost: 1.6010290384292603, error rate: 0.482\n",
            "i: 4 j: 640, nb: 1303, cost: 1.5780441761016846, error rate: 0.487\n",
            "i: 4 j: 660, nb: 1303, cost: 1.6226286888122559, error rate: 0.488\n",
            "i: 4 j: 680, nb: 1303, cost: 1.680305004119873, error rate: 0.523\n",
            "i: 4 j: 700, nb: 1303, cost: 1.594983696937561, error rate: 0.489\n",
            "i: 4 j: 720, nb: 1303, cost: 1.6144306659698486, error rate: 0.488\n",
            "i: 4 j: 740, nb: 1303, cost: 1.6312074661254883, error rate: 0.496\n",
            "i: 4 j: 760, nb: 1303, cost: 1.6089199781417847, error rate: 0.481\n",
            "i: 4 j: 780, nb: 1303, cost: 1.663312315940857, error rate: 0.506\n",
            "i: 4 j: 800, nb: 1303, cost: 1.5922476053237915, error rate: 0.486\n",
            "i: 4 j: 820, nb: 1303, cost: 1.5702890157699585, error rate: 0.47\n",
            "i: 4 j: 840, nb: 1303, cost: 1.601671576499939, error rate: 0.492\n",
            "i: 4 j: 860, nb: 1303, cost: 1.6211845874786377, error rate: 0.485\n",
            "i: 4 j: 880, nb: 1303, cost: 1.5584230422973633, error rate: 0.464\n",
            "i: 4 j: 900, nb: 1303, cost: 1.6076080799102783, error rate: 0.477\n",
            "i: 4 j: 920, nb: 1303, cost: 1.6051491498947144, error rate: 0.489\n",
            "i: 4 j: 940, nb: 1303, cost: 1.6275758743286133, error rate: 0.499\n",
            "i: 4 j: 960, nb: 1303, cost: 1.5907708406448364, error rate: 0.471\n",
            "i: 4 j: 980, nb: 1303, cost: 1.5787169933319092, error rate: 0.469\n",
            "i: 4 j: 1000, nb: 1303, cost: 1.5640387535095215, error rate: 0.461\n",
            "i: 4 j: 1020, nb: 1303, cost: 1.5751678943634033, error rate: 0.466\n",
            "i: 4 j: 1040, nb: 1303, cost: 1.6204090118408203, error rate: 0.494\n",
            "i: 4 j: 1060, nb: 1303, cost: 1.6347427368164062, error rate: 0.492\n",
            "i: 4 j: 1080, nb: 1303, cost: 1.6067973375320435, error rate: 0.483\n",
            "i: 4 j: 1100, nb: 1303, cost: 1.5946062803268433, error rate: 0.469\n",
            "i: 4 j: 1120, nb: 1303, cost: 1.6031997203826904, error rate: 0.468\n",
            "i: 4 j: 1140, nb: 1303, cost: 1.5912752151489258, error rate: 0.48\n",
            "i: 4 j: 1160, nb: 1303, cost: 1.5734002590179443, error rate: 0.468\n",
            "i: 4 j: 1180, nb: 1303, cost: 1.5855820178985596, error rate: 0.487\n",
            "i: 4 j: 1200, nb: 1303, cost: 1.6443772315979004, error rate: 0.518\n",
            "i: 4 j: 1220, nb: 1303, cost: 1.612504482269287, error rate: 0.477\n",
            "i: 4 j: 1240, nb: 1303, cost: 1.592024326324463, error rate: 0.498\n",
            "i: 4 j: 1260, nb: 1303, cost: 1.634157657623291, error rate: 0.482\n",
            "i: 4 j: 1280, nb: 1303, cost: 1.5376895666122437, error rate: 0.459\n",
            "i: 4 j: 1300, nb: 1303, cost: 1.5633641481399536, error rate: 0.473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndEVRlEHC/wdH84oqOBYu1wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3etxOuzKuY58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y0wVReguY98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw4otP-yuY__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DoXRFtouY8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}