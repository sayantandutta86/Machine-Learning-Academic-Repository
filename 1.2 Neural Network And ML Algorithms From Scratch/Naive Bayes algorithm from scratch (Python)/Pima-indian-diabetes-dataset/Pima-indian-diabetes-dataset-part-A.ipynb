{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "\n",
    "\n",
    "## Diabetes Classification\n",
    "\n",
    "A famous collection of data on whether a patient has diabetes, known as the Pima Indians dataset, and originally owned by the National Institute of Diabetes and Digestive and Kidney Diseases can be found at Kaggle. Download this dataset from https://www.kaggle.com/kumargh/pimaindiansdiabetescsv. This data has a set of attributes of patients, and a categorical variable telling whether the patient is diabetic or not. For several attributes in this data set, a value of 0 may indicate a missing value of the variable. There are a total of 767 data-points.\n",
    "\n",
    "### Question: Part-A\n",
    "\n",
    "Build a simple naive Bayes classifier to classify this data set. Use a normal distribution to model each of the class-conditional distributions.\n",
    "\n",
    "Compute an estimate of the accuracy of the classifier by averaging over 10 test-train splits. Each split should randomly assign 20% of the data to test, and the rest to train.\n",
    "Write this classifier and the test-train split code from scratch.  Libraries can only be used to load & hold the data.\n",
    "\n",
    "### Question: Part-B\n",
    "\n",
    "Now adjust your code so that, for attribute 3 (Diastolic blood pressure), attribute 4 (Triceps skinfold thickness), attribute 6 (Body mass index), and attribute 8 (Age), it regards a value of 0 as a missing value when estimating the class-conditional distributions, and the posterior. \n",
    "\n",
    "Compute an estimate of the accuracy of the classifier by averaging over 10 test-train splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 accuracy: 0.7857142857142857 \n",
      "Fold 2 accuracy: 0.7077922077922078 \n",
      "Fold 3 accuracy: 0.7532467532467533 \n",
      "Fold 4 accuracy: 0.7662337662337663 \n",
      "Fold 5 accuracy: 0.7857142857142857 \n",
      "Fold 6 accuracy: 0.7402597402597403 \n",
      "Fold 7 accuracy: 0.7597402597402597 \n",
      "Fold 8 accuracy: 0.7792207792207793 \n",
      "Fold 9 accuracy: 0.7597402597402597 \n",
      "Fold 10 accuracy: 0.7467532467532467 \n",
      "Mean Accuracy over 10 train-test splits: 0.7584415584415585\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "\n",
    "#Load Dataset \n",
    "def loadfile(file):\n",
    "    lines = csv.reader(open(file))\n",
    "    datafile = list(lines)\n",
    "    for i in range(len(datafile)):\n",
    "        datafile[i] = [float(x) for x in datafile[i]]\n",
    "    return datafile\n",
    "\n",
    "#Train Test Split\n",
    "def train_test_split(datafile, split_ratio):\n",
    "    train_size = int(len(datafile) * split_ratio)\n",
    "    train_data = []\n",
    "    datafile_copy = list(datafile)\n",
    "    while len(train_data) < train_size:\n",
    "        index = random.randrange(len(datafile_copy))\n",
    "        train_data.append(datafile_copy.pop(index))\n",
    "    return [train_data, datafile_copy]\n",
    "\n",
    "#Create a dictionary separating class and data\n",
    "def class_data_dict(datafile):\n",
    "    class_dict = {}\n",
    "    for i in range(len(datafile)):\n",
    "        row = datafile[i]\n",
    "        if (row[-1] not in class_dict):\n",
    "            class_dict[row[-1]] = []\n",
    "        class_dict[row[-1]].append(row)\n",
    "    return class_dict\n",
    "\n",
    "#Calculate Mean and Standard Deviation\n",
    "import math\n",
    "def mean(values):\n",
    "    return float(sum(values)/float(len(values)))\n",
    " \n",
    "def stdev(values):\n",
    "    avg = mean(values)\n",
    "    sum_of_sq = float (sum([pow(x-avg,2) for x in values]))\n",
    "    length = float (len(values)-1)\n",
    "    variance = float (sum_of_sq/length)\n",
    "    return math.sqrt(variance) \n",
    "\n",
    "#mean, standard deviation, and attributes\n",
    "def condense(datafile):\n",
    "    condensed_values = [(mean(features), stdev(features)) for features in zip(*datafile)]\n",
    "    del condensed_values[-1]\n",
    "    return condensed_values\n",
    "\n",
    "#Separate class and attribute properties like mean, std \n",
    "def condense_class_dict(datafile):\n",
    "    class_dict = class_data_dict(datafile)\n",
    "    condense_class = {}\n",
    "    for class_value, data in class_dict.items():\n",
    "        condense_class[class_value] = condense(data)\n",
    "    return condense_class\n",
    "\n",
    "#calculateProbability\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "\n",
    "def calc_log_probability(x, mean, stdev):\n",
    "    return np.log(norm.pdf(x, mean, stdev))\n",
    "\n",
    "def calc_only_class_log_prob(train_data):\n",
    "    class_dict = class_data_dict(train_data)\n",
    "    class_count = {}\n",
    "    total_count = 0\n",
    "    class_only_log_prob = {}\n",
    "    for class_value in class_dict.keys():\n",
    "        class_count[class_value] = len(class_dict[class_value])\n",
    "    for x in class_count.values():\n",
    "        total_count += x\n",
    "    for class_value,count in class_count.items():\n",
    "        y = count/total_count\n",
    "        log_prob = np.log(y)\n",
    "        class_only_log_prob[class_value] = log_prob\n",
    "    return class_only_log_prob        \n",
    "\n",
    "#calculateClassProbabilities ***log of probability***\n",
    "def calc_class_probabilities(condensed_values, row, train_data):\n",
    "    class_log_probabilities = {} \n",
    "    class_only_log_prob = calc_only_class_log_prob(train_data)\n",
    "    for class_val, attribute_values in condensed_values.items():   \n",
    "        class_only_prob = class_only_log_prob[class_val]\n",
    "        class_log_probabilities[class_val] = 0\n",
    "        for i in range(len(attribute_values)):\n",
    "            mean, stdev = attribute_values[i]\n",
    "            x = row[i]\n",
    "            class_log_probabilities[class_val] += calc_log_probability(x, mean, stdev)\n",
    "        class_log_probabilities[class_val] += class_only_prob\n",
    "    return class_log_probabilities\n",
    "\n",
    "#predict\n",
    "def class_prediction(condensed_values, row, train_data):\n",
    "    probabilities = calc_class_probabilities(condensed_values, row, train_data)\n",
    "    best_class, highest_Prob = None, -1\n",
    "    for class_val, probability in probabilities.items():\n",
    "        if best_class is None or probability > highest_Prob:\n",
    "            highest_Prob = probability\n",
    "            best_class = class_val\n",
    "    return best_class\n",
    "\n",
    "#getPredictions(summaries, testSet):\n",
    "def predict_test_data(condensed_values, train_data, test_data):\n",
    "    test_predictions = []\n",
    "    for i in range(len(test_data)):\n",
    "        result = class_prediction(condensed_values, test_data[i], train_data)\n",
    "        test_predictions.append(result)\n",
    "    return test_predictions\n",
    "\n",
    "#getAccuracy\n",
    "def prediction_accuracy(test_data, test_predictions):\n",
    "    correct = 0\n",
    "    for x in range(len(test_data)):\n",
    "        if test_data[x][-1] == test_predictions[x]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(test_data)))\n",
    "\n",
    "def start():\n",
    "    file = 'pima-indians-diabetes.csv'\n",
    "    split_ratio = 0.80\n",
    "    k_fold = 10\n",
    "    datafile = loadfile(file)\n",
    "    actual_length = len(datafile)\n",
    "    datafile_copy = list(datafile)\n",
    "    accuracy_list = []\n",
    "    for i in range(k_fold):\n",
    "        train_data, test_data = train_test_split(datafile_copy, split_ratio)\n",
    "        #print(f'Split {actual_length} rows into train={len(train_data)} and test={len(test_data)} rows')\n",
    "        # prepare model\n",
    "        condense_class = condense_class_dict(train_data)\n",
    "        # test model\n",
    "        test_predictions = predict_test_data(condense_class, train_data, test_data)\n",
    "        accuracy = prediction_accuracy(test_data, test_predictions)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print(f'Fold {i+1} accuracy: {accuracy} ')\n",
    "       \n",
    "    mean_accuracy = float (sum(accuracy_list)/len(accuracy_list))\n",
    "    print(f'Mean Accuracy over 10 train-test splits: {mean_accuracy}')\n",
    "    \n",
    "start()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
