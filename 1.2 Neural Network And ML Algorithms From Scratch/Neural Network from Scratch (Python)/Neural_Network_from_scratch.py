# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18RSkH7HSoOWOEttipo5ZrAEXhO5bTr_i

#Sayantan Dutta

Implement and train a neural network from scratch in Python for the MNIST dataset (no PyTorch). The neural network should be trained on the Training Set using stochastic gradient descent. It should achieve 97-98% accuracy on the Test Set.
"""

#import required libraries

import numpy as np
import h5py

#Functions

#Activation Function - Sigmoid function
def sigmoid_function(z):
  
  return 1./(1. + np.exp(-z))


#Loss function
def calculate_loss(y, y_pred):
  
  x = y.shape[1]
  
  return - (1./x) * np.sum(np.multiply(y, np.log(y_pred)))

#Feed-forward pass
def feedforward(X, parameter_dict):
  
  weight_dict = {}
  
  weight_dict['lf1'] = np.matmul(parameter_dict['w1'], X) + parameter_dict['b1']
  
  weight_dict['z1'] = sigmoid_function(weight_dict['lf1']) #applying sigmoid
  
  weight_dict['lf2'] = np.matmul(parameter_dict['w2'], weight_dict['z1']) + parameter_dict['b2']
  
  weight_dict['z2'] = np.exp(weight_dict['lf2']) / np.sum(np.exp(weight_dict['lf2']), axis = 0) #applying softmax
  
  return weight_dict

#Backpropagation
def backpropagation(X, y, parameter_dict, weight_dict, batch_size):
  
  #gradient at output layer
  
  dz2 = weight_dict['z2'] - y #error at output layer
  
  dw2 = (1./batch_size) * np.matmul(dz2, weight_dict['z1'].T)
  
  db2 = (1./batch_size) * np.sum(dz2, axis = 1, keepdims = True)
  
  #gradient calculation at hidden layer
  
  dz1 = np.matmul(parameter_dict['w2'].T, dz2)
  
  dlf1 = dz1 * weight_dict['z1'] * (1 - weight_dict['z1'])
  
  dw1 = (1./batch_size) * np.matmul(dlf1, X.T)
  
  db1 = (1./batch_size) * np.sum(dlf1, axis=1, keepdims = True)
  
  gradients = {'dw1' : dw1, 'db1' : db1, 'dw2' : dw2, 'db2' : db2}
  
  return gradients

#Predict accuracy
def predict(y_pred_list, y_test):
  
  total_correct = 0
  
  total_records = y_test.shape[1]
    
  for i in range(total_records):
    
    y_pred = np.argmax(y_pred_list.T[i])
    
    y_actual = np.argmax(y_test.T[i])
      
    if y_pred == y_actual:
      
      total_correct += 1
      
  return np.round(total_correct/total_records * 100, 4)



#Load dataset
data = h5py.File('MNISTdata.hdf5', 'r')

#Load train and test data

X_train = np.float32(data['x_train'])

X_test = np.float32(data['x_test'])

y_train = np.int32(np.array(data['y_train'])).reshape(-1,1)

y_test = np.int32(np.array(data['y_test'])).reshape(-1,1)

data.close()

train_records = y_train.shape[0]

test_records = y_test.shape[0]

#one-hot encode output classes

y = np.vstack((y_train, y_test))

total_records = y.shape[0]

y = y.reshape(1, total_records)

classes = len(np.unique(y))

y_new = np.eye(classes)[y]

y_new = y_new.T.reshape(classes, total_records)

# shuffle the training set

X_train = X_train.T

X_test = X_test.T

y_train = y_new[:, :train_records]

y_test = y_new[:, train_records:]

index_shuffled = np.random.permutation(train_records)

X_train, y_train = X_train[:, index_shuffled], y_train[:, index_shuffled]

#define hyper-parameters

#learning rate
lr=0.5

#epochs
epochs = 20

#input units
input_units = 784

#hidden units in hidden_layer_1
hidden_units = 100

#output_units
output_units = 10

#batch_size
batch_size = 16

#initialize weights and variance calibrated by square root of input units
parameter_dict = {"w1" : np.random.randn(hidden_units, input_units) * np.sqrt(1./input_units),
                  "b1" : np.random.randn(hidden_units,1) * np.sqrt(1./input_units),
                  "w2" : np.random.randn(output_units, hidden_units) * np.sqrt(1./hidden_units),
                  "b2" : np.random.randn(output_units,1) * np.sqrt(1./hidden_units)}


#Training the neural network, and test on the test data

for epoch in range(epochs):
  
    if (epoch <= 0.3 * epochs) : 
        lr = 0.5

    elif ((epoch > 0.3 * epochs) & (epoch <= 0.8 * epochs)): 
          lr = 0.1

    elif (epoch > 0.8 * epochs): 
          lr = 0.01

    
    shuffled_index = np.random.permutation(X_train.shape[1])

    X_train_new = X_train[:, shuffled_index]

    y_train_new = y_train[:, shuffled_index]

    number_of_batch = int (X_train.shape[1] / batch_size)

    for i in range(number_of_batch):

        start = i * batch_size

        end = min(start + batch_size, X_train.shape[1] - 1)

        X_batch = X_train_new[:, start:end]

        y_batch = y_train_new[:, start:end]

        #feedforward    
        weight_dict = feedforward(X_batch, parameter_dict)

        #calculate gradients
        gradients = backpropagation(X_batch, y_batch, parameter_dict, weight_dict, end-start)


        #gradients 
        dw1 = gradients['dw1']

        db1 = gradients['db1']

        dw2 = gradients['dw2']

        db2 = gradients['db2']

        #SGD
        parameter_dict['w1'] = parameter_dict['w1'] - lr * dw1

        parameter_dict['b1'] = parameter_dict['b1'] - lr * db1

        parameter_dict['w2'] = parameter_dict['w2'] - lr * dw2

        parameter_dict['b2'] = parameter_dict['b2'] - lr * db2
    
    
    
    #forward pass on the training set based on updated parameters

    weight_dict_train = feedforward(X_train, parameter_dict)

    train_loss = calculate_loss(y_train, weight_dict_train["z2"])

    #forward pass on the test set based on updated parameters

    weight_dict_test = feedforward(X_test, parameter_dict)

    test_loss = calculate_loss(y_test, weight_dict_test['z2']) 

    if (epoch+1) % 5 == 0:
      
      print(f'Epoch {epoch+1}: Training loss = {train_loss}, Test loss = {test_loss}')
      
      print(f'Accuracy on train set is {predict(weight_dict_train["z2"], y_train)} %') 
      
      print(f'Accuracy on test set is {predict(weight_dict_test["z2"], y_test)} %') 
      
      print()