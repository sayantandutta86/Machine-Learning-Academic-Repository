{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glove_implement_from_scratch_tf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIRNKook7DvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HWgyUMfrGun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "#os.chdir('/content/drive/My Drive/Colab Notebooks/Lazy courses/NLP2/pretrained_data')\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/Lazy courses/NLP2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxcE8VOw6-pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from datetime import datetime\n",
        "from sklearn.utils import shuffle \n",
        "from utils import find_analogies\n",
        "\n",
        "from utils import get_wikipedia_data\n",
        "from brown import get_sentences_with_word2idx_limit_vocab, get_sentences_with_word2idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfOzauyN6-r0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ALS minimizes two loss functions alternatively; \n",
        "#It first holds user matrix fixed and runs gradient descent with item matrix; \n",
        "#then it holds item matrix fixed and runs gradient descent with user matrix\n",
        "#Using ALS whats the least number of files to get correct analogies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfyzsvTU6-wo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Glove:\n",
        "    def __init__(self, D, V, context_sz):\n",
        "        self.D = D\n",
        "        self.V = V\n",
        "        self.context_sz = context_sz\n",
        "\n",
        "    def fit(self, sentences, cc_matrix=None, learning_rate=1e-4, reg=0.1, xmax=100, alpha=0.75, epochs=10, gd=False):\n",
        "        #build co-occurance matrix\n",
        "        t0 = datetime.now()\n",
        "        V = self.V\n",
        "        D = self.D\n",
        "\n",
        "        if not os.path.exists(cc_matrix):\n",
        "            X = np.zeros((V, V))\n",
        "            N = len(sentences)\n",
        "            print(\"Number of senetences to process\", N)\n",
        "            it = 0\n",
        "\n",
        "            for sentence in sentences:\n",
        "                it += 1\n",
        "                if it % 10000 == 0:\n",
        "                    print(f\"Processed {it} / {N}\")\n",
        "                n =len(sentence)\n",
        "                for i in range(n):\n",
        "                    #i points to which element of the sentence we are looking at\n",
        "                    wi = sentence[i]\n",
        "\n",
        "                    start = max(0, i - self.context_sz) \n",
        "                    end = min(n, i + self.context_sz)\n",
        "\n",
        "                    #we can either choose only one side as context, or both\n",
        "                    # Here we are choosing both\n",
        "                    #we have to make sure 'start' and 'end' tokens are part of\n",
        "                    #some context otherwise their f(X) will be 0 \n",
        "                    #(denominator in bias update)\n",
        "\n",
        "                    if i - self.context_sz < 0:\n",
        "                        points = 1.0 / (i+1)\n",
        "                        X[wi,0] += points\n",
        "                        X[0,wi] += points\n",
        "\n",
        "                    if i + self.context_sz > 0:\n",
        "                        points = 1.0 / (n-i)\n",
        "                        X[wi,1]  += points\n",
        "                        X[1,wi] += points\n",
        "\n",
        "                    #left side \n",
        "                    for j in range(start, i):\n",
        "                        wj = sentence[j]\n",
        "                        points = 1.0 / (i - j) #this is +ve\n",
        "                        X[wi, wj] += points\n",
        "                        X[wj, wi] += points\n",
        "                    #right side    \n",
        "                    for j in range(i+1, end):\n",
        "                        wj = sentence[j]\n",
        "                        points = 1.0 /(j - i) #this is +ve\n",
        "                        X[wi, wj] += points\n",
        "                        X[wj, wi] += points\n",
        "\n",
        "            np.save(cc_matrix, X)\n",
        "        else:\n",
        "            X = np.load(cc_matrix)\n",
        "\n",
        "        print(\"max in X\", X.max())\n",
        "\n",
        "        #weight normalization\n",
        "        fX = np.zeros((V, V))\n",
        "        fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n",
        "        fX[X >= xmax] = 1\n",
        "\n",
        "        print(\"max in f(X)\", fX.max())\n",
        "\n",
        "        #target\n",
        "        logX = np.log(X + 1)\n",
        "\n",
        "        print(\"max in log(X):\", logX.max())\n",
        "\n",
        "        print(\"Building co-occurance matrix:\", (datetime.now()- t0))\n",
        "\n",
        "        #initialize weights\n",
        "        W = np.random.randn(V, D) / np.sqrt(V + D)\n",
        "        b = np.zeros(V)\n",
        "        U = np.random.randn(V,D) / np.sqrt(V + D)\n",
        "        c = np.zeros(V)\n",
        "        mu = logX.mean()\n",
        "\n",
        "        #initialize weights, inputs, target placeholder\n",
        "        tfW = tf.Variable(W.astype(np.float32))\n",
        "        tfb = tf.Variable(b.reshape(V,1).astype(np.float32))\n",
        "        tfU = tf.Variable(U.astype(np.float32))\n",
        "        tfc = tf.Variable(c.reshape(1,V).astype(np.float32))\n",
        "        tfLogX = tf.placeholder(tf.float32, shape=(V, V))\n",
        "        tffX= tf.placeholder(tf.float32, shape=(V, V))\n",
        "        \n",
        "        delta = tf.matmul(tfW, tf.transpose(tfU)) + tfb + tfc + mu - tfLogX\n",
        "        cost = tf.reduce_sum(tffX * delta * delta)\n",
        "        regularized_cost = cost\n",
        "        for param in (tfW, tfU):\n",
        "            regularized_cost += reg * tf.reduce_sum(param * param)\n",
        "\n",
        "        train_op = tf.train.MomentumOptimizer(learning_rate, momentum=0.9\n",
        "                                              ).minimize(regularized_cost)\n",
        "        #train_op = tf.train.AdamOptimizer(1e-3).minimize(regularized_cost)                                      \n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "        session = tf.InteractiveSession()\n",
        "        session.run(init)\n",
        "\n",
        "\n",
        "        costs = []\n",
        "        sentence_indexes = range(len(sentences))\n",
        "        for epoch in range(epochs):\n",
        "            c, _ = session.run((cost, train_op), feed_dict = {tfLogX:logX, tffX:fX})\n",
        "            print(\"Epoch:\", epoch, \"Cost\", c)\n",
        "            costs.append(c)\n",
        "\n",
        "        #save for future calculations\n",
        "        self.W, self.U = session.run([tfW, tfU])\n",
        "\n",
        "        plt.plot(costs)\n",
        "        plt.show()\n",
        "\n",
        "    def save(self, fn):\n",
        "        # function word_analogies expects a (V,D) matrx and a (D,V) matrix\n",
        "        arrays = [self.W, self.U.T]\n",
        "        np.savez(fn, *arrays)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4oNM6lb6-zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(we_file, w2i_file, use_brown=True, n_files=50):\n",
        "    if use_brown:\n",
        "        cc_matrix = \"cc_matrix_brown.npy\"\n",
        "    else:\n",
        "        cc_matrix = \"cc_matrix_%s.npy\" % n_files\n",
        "\n",
        "    # hacky way of checking if we need to re-load the raw data or not\n",
        "    # remember, only the co-occurrence matrix is needed for training\n",
        "    if os.path.exists(cc_matrix):\n",
        "        with open(w2i_file) as f:\n",
        "            word2idx = json.load(f)\n",
        "        sentences = [] # dummy - we won't actually use it\n",
        "    else:\n",
        "        if use_brown:\n",
        "            keep_words = set([\n",
        "                'king', 'man', 'woman',\n",
        "                'france', 'paris', 'london', 'rome', 'italy', 'britain', 'england',\n",
        "                'french', 'english', 'japan', 'japanese', 'chinese', 'italian',\n",
        "                'australia', 'australian', 'december', 'november', 'june',\n",
        "                'january', 'february', 'march', 'april', 'may', 'july', 'august',\n",
        "                'september', 'october',\n",
        "            ])\n",
        "            sentences, word2idx = get_sentences_with_word2idx_limit_vocab(n_vocab=5000, keep_words=keep_words)\n",
        "        else:\n",
        "            sentences, word2idx = get_wikipedia_data(n_files=n_files, n_vocab=2000)\n",
        "        \n",
        "        with open(w2i_file, 'w') as f:\n",
        "            json.dump(word2idx, f)\n",
        "\n",
        "    V = len(word2idx)\n",
        "    model = Glove(100, V, 10)\n",
        "    model.fit(sentences, cc_matrix=cc_matrix, epochs=200)\n",
        "    model.save(we_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEm59AL96-2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3529cd21-bc08-41c3-e822-f1620ef9e671"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    we = 'glove_model_50_tf.npz'\n",
        "    w2i = 'glove_word2idx_50_tf.json'\n",
        "    main(we, w2i, use_brown=False)\n",
        "\n",
        "    # load back embeddings\n",
        "    npz = np.load(we)\n",
        "    W1 = npz['arr_0']\n",
        "    W2 = npz['arr_1']\n",
        "\n",
        "    with open(w2i) as f:\n",
        "        word2idx = json.load(f)\n",
        "        idx2word = {i:w for w,i in word2idx.items()}\n",
        "\n",
        "    for concat in (True, False):\n",
        "        print(\"** concat:\", concat)\n",
        "\n",
        "        if concat:\n",
        "            We = np.hstack([W1, W2.T])\n",
        "        else:\n",
        "            We = (W1 + W2.T) / 2\n",
        "\n",
        "\n",
        "        find_analogies('king', 'man', 'woman', We, word2idx, idx2word)\n",
        "        find_analogies('france', 'paris', 'london', We, word2idx, idx2word)\n",
        "        find_analogies('france', 'paris', 'rome', We, word2idx, idx2word)\n",
        "        find_analogies('paris', 'france', 'italy', We, word2idx, idx2word)\n",
        "        find_analogies('france', 'french', 'english', We, word2idx, idx2word)\n",
        "        find_analogies('japan', 'japanese', 'chinese', We, word2idx, idx2word)\n",
        "        find_analogies('japan', 'japanese', 'italian', We, word2idx, idx2word)\n",
        "        find_analogies('japan', 'japanese', 'australian', We, word2idx, idx2word)\n",
        "        find_analogies('december', 'november', 'june', We, word2idx, idx2word)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max in X 54309720.082714856\n",
            "max in f(X) 1.0\n",
            "max in log(X): 17.81021379434538\n",
            "Building co-occurance matrix: 0:00:00.589964\n",
            "Epoch: 0 Cost 5300739.0\n",
            "Epoch: 1 Cost 2499207.5\n",
            "Epoch: 2 Cost 845355.1\n",
            "Epoch: 3 Cost 1960197.8\n",
            "Epoch: 4 Cost 3452899.0\n",
            "Epoch: 5 Cost 3005471.0\n",
            "Epoch: 6 Cost 1475948.5\n",
            "Epoch: 7 Cost 1008910.6\n",
            "Epoch: 8 Cost 1813775.2\n",
            "Epoch: 9 Cost 2314915.0\n",
            "Epoch: 10 Cost 1732180.0\n",
            "Epoch: 11 Cost 994187.44\n",
            "Epoch: 12 Cost 1068871.0\n",
            "Epoch: 13 Cost 1586678.5\n",
            "Epoch: 14 Cost 1628897.5\n",
            "Epoch: 15 Cost 1136546.0\n",
            "Epoch: 16 Cost 827649.44\n",
            "Epoch: 17 Cost 1031575.6\n",
            "Epoch: 18 Cost 1297668.8\n",
            "Epoch: 19 Cost 1179570.0\n",
            "Epoch: 20 Cost 863114.0\n",
            "Epoch: 21 Cost 783256.7\n",
            "Epoch: 22 Cost 963821.25\n",
            "Epoch: 23 Cost 1062579.8\n",
            "Epoch: 24 Cost 922341.6\n",
            "Epoch: 25 Cost 755604.2\n",
            "Epoch: 26 Cost 773451.5\n",
            "Epoch: 27 Cost 887721.1\n",
            "Epoch: 28 Cost 891972.25\n",
            "Epoch: 29 Cost 775066.8\n",
            "Epoch: 30 Cost 700062.6\n",
            "Epoch: 31 Cost 741108.75\n",
            "Epoch: 32 Cost 794677.44\n",
            "Epoch: 33 Cost 757894.75\n",
            "Epoch: 34 Cost 673228.9\n",
            "Epoch: 35 Cost 642098.7\n",
            "Epoch: 36 Cost 671766.6\n",
            "Epoch: 37 Cost 683033.4\n",
            "Epoch: 38 Cost 637900.5\n",
            "Epoch: 39 Cost 585583.9\n",
            "Epoch: 40 Cost 577528.1\n",
            "Epoch: 41 Cost 595059.56\n",
            "Epoch: 42 Cost 588664.1\n",
            "Epoch: 43 Cost 552737.3\n",
            "Epoch: 44 Cost 524307.8\n",
            "Epoch: 45 Cost 522357.25\n",
            "Epoch: 46 Cost 524739.7\n",
            "Epoch: 47 Cost 507826.2\n",
            "Epoch: 48 Cost 480550.75\n",
            "Epoch: 49 Cost 465687.28\n",
            "Epoch: 50 Cost 464955.88\n",
            "Epoch: 51 Cost 460497.78\n",
            "Epoch: 52 Cost 443813.88\n",
            "Epoch: 53 Cost 426042.3\n",
            "Epoch: 54 Cost 418524.16\n",
            "Epoch: 55 Cost 416723.7\n",
            "Epoch: 56 Cost 409592.88\n",
            "Epoch: 57 Cost 396318.1\n",
            "Epoch: 58 Cost 385377.44\n",
            "Epoch: 59 Cost 380625.88\n",
            "Epoch: 60 Cost 376615.38\n",
            "Epoch: 61 Cost 368071.44\n",
            "Epoch: 62 Cost 357400.0\n",
            "Epoch: 63 Cost 349898.3\n",
            "Epoch: 64 Cost 345757.34\n",
            "Epoch: 65 Cost 340804.44\n",
            "Epoch: 66 Cost 333275.12\n",
            "Epoch: 67 Cost 325854.5\n",
            "Epoch: 68 Cost 320978.1\n",
            "Epoch: 69 Cost 317348.8\n",
            "Epoch: 70 Cost 312424.34\n",
            "Epoch: 71 Cost 306247.9\n",
            "Epoch: 72 Cost 300917.94\n",
            "Epoch: 73 Cost 297232.2\n",
            "Epoch: 74 Cost 293779.22\n",
            "Epoch: 75 Cost 289320.53\n",
            "Epoch: 76 Cost 284493.88\n",
            "Epoch: 77 Cost 280558.5\n",
            "Epoch: 78 Cost 277501.22\n",
            "Epoch: 79 Cost 274289.56\n",
            "Epoch: 80 Cost 270507.25\n",
            "Epoch: 81 Cost 266804.5\n",
            "Epoch: 82 Cost 263758.56\n",
            "Epoch: 83 Cost 261060.28\n",
            "Epoch: 84 Cost 258111.73\n",
            "Epoch: 85 Cost 254918.56\n",
            "Epoch: 86 Cost 251959.12\n",
            "Epoch: 87 Cost 249413.12\n",
            "Epoch: 88 Cost 246963.03\n",
            "Epoch: 89 Cost 244337.5\n",
            "Epoch: 90 Cost 241684.28\n",
            "Epoch: 91 Cost 239276.64\n",
            "Epoch: 92 Cost 237098.16\n",
            "Epoch: 93 Cost 234917.27\n",
            "Epoch: 94 Cost 232652.22\n",
            "Epoch: 95 Cost 230454.86\n",
            "Epoch: 96 Cost 228448.58\n",
            "Epoch: 97 Cost 226557.06\n",
            "Epoch: 98 Cost 224647.62\n",
            "Epoch: 99 Cost 222725.08\n",
            "Epoch: 100 Cost 220893.25\n",
            "Epoch: 101 Cost 219185.1\n",
            "Epoch: 102 Cost 217527.75\n",
            "Epoch: 103 Cost 215864.97\n",
            "Epoch: 104 Cost 214231.62\n",
            "Epoch: 105 Cost 212684.62\n",
            "Epoch: 106 Cost 211216.12\n",
            "Epoch: 107 Cost 209776.38\n",
            "Epoch: 108 Cost 208351.56\n",
            "Epoch: 109 Cost 206974.94\n",
            "Epoch: 110 Cost 205668.12\n",
            "Epoch: 111 Cost 204410.12\n",
            "Epoch: 112 Cost 203173.38\n",
            "Epoch: 113 Cost 201962.56\n",
            "Epoch: 114 Cost 200800.16\n",
            "Epoch: 115 Cost 199689.23\n",
            "Epoch: 116 Cost 198610.75\n",
            "Epoch: 117 Cost 197553.05\n",
            "Epoch: 118 Cost 196525.53\n",
            "Epoch: 119 Cost 195539.86\n",
            "Epoch: 120 Cost 194591.52\n",
            "Epoch: 121 Cost 193667.88\n",
            "Epoch: 122 Cost 192766.25\n",
            "Epoch: 123 Cost 191894.53\n",
            "Epoch: 124 Cost 191056.52\n",
            "Epoch: 125 Cost 190246.02\n",
            "Epoch: 126 Cost 189456.1\n",
            "Epoch: 127 Cost 188687.81\n",
            "Epoch: 128 Cost 187945.89\n",
            "Epoch: 129 Cost 187230.14\n",
            "Epoch: 130 Cost 186535.53\n",
            "Epoch: 131 Cost 185859.16\n",
            "Epoch: 132 Cost 185202.81\n",
            "Epoch: 133 Cost 184568.61\n",
            "Epoch: 134 Cost 183954.83\n",
            "Epoch: 135 Cost 183358.14\n",
            "Epoch: 136 Cost 182777.67\n",
            "Epoch: 137 Cost 182214.88\n",
            "Epoch: 138 Cost 181670.14\n",
            "Epoch: 139 Cost 181141.6\n",
            "Epoch: 140 Cost 180627.45\n",
            "Epoch: 141 Cost 180127.67\n",
            "Epoch: 142 Cost 179642.97\n",
            "Epoch: 143 Cost 179172.9\n",
            "Epoch: 144 Cost 178716.0\n",
            "Epoch: 145 Cost 178271.39\n",
            "Epoch: 146 Cost 177839.25\n",
            "Epoch: 147 Cost 177419.69\n",
            "Epoch: 148 Cost 177012.0\n",
            "Epoch: 149 Cost 176615.19\n",
            "Epoch: 150 Cost 176228.94\n",
            "Epoch: 151 Cost 175853.34\n",
            "Epoch: 152 Cost 175488.19\n",
            "Epoch: 153 Cost 175132.78\n",
            "Epoch: 154 Cost 174786.56\n",
            "Epoch: 155 Cost 174449.38\n",
            "Epoch: 156 Cost 174121.17\n",
            "Epoch: 157 Cost 173801.62\n",
            "Epoch: 158 Cost 173490.22\n",
            "Epoch: 159 Cost 173186.58\n",
            "Epoch: 160 Cost 172890.62\n",
            "Epoch: 161 Cost 172602.25\n",
            "Epoch: 162 Cost 172321.1\n",
            "Epoch: 163 Cost 172046.78\n",
            "Epoch: 164 Cost 171779.12\n",
            "Epoch: 165 Cost 171518.0\n",
            "Epoch: 166 Cost 171263.25\n",
            "Epoch: 167 Cost 171014.58\n",
            "Epoch: 168 Cost 170771.75\n",
            "Epoch: 169 Cost 170534.56\n",
            "Epoch: 170 Cost 170302.97\n",
            "Epoch: 171 Cost 170076.75\n",
            "Epoch: 172 Cost 169855.72\n",
            "Epoch: 173 Cost 169639.62\n",
            "Epoch: 174 Cost 169428.45\n",
            "Epoch: 175 Cost 169222.0\n",
            "Epoch: 176 Cost 169020.16\n",
            "Epoch: 177 Cost 168822.69\n",
            "Epoch: 178 Cost 168629.53\n",
            "Epoch: 179 Cost 168440.56\n",
            "Epoch: 180 Cost 168255.66\n",
            "Epoch: 181 Cost 168074.66\n",
            "Epoch: 182 Cost 167897.48\n",
            "Epoch: 183 Cost 167724.0\n",
            "Epoch: 184 Cost 167554.12\n",
            "Epoch: 185 Cost 167387.75\n",
            "Epoch: 186 Cost 167224.78\n",
            "Epoch: 187 Cost 167065.06\n",
            "Epoch: 188 Cost 166908.6\n",
            "Epoch: 189 Cost 166755.22\n",
            "Epoch: 190 Cost 166604.88\n",
            "Epoch: 191 Cost 166457.47\n",
            "Epoch: 192 Cost 166312.94\n",
            "Epoch: 193 Cost 166171.19\n",
            "Epoch: 194 Cost 166032.17\n",
            "Epoch: 195 Cost 165895.77\n",
            "Epoch: 196 Cost 165761.94\n",
            "Epoch: 197 Cost 165630.6\n",
            "Epoch: 198 Cost 165501.69\n",
            "Epoch: 199 Cost 165375.17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "** concat: True\n",
            "Closest match by euclidean distance: queen\n",
            "king - man = queen - woman\n",
            "Closest match by cosine distance: emperor\n",
            "king - man = emperor - woman\n",
            "Closest match by euclidean distance: england\n",
            "france - paris = england - london\n",
            "Closest match by cosine distance: england\n",
            "france - paris = england - london\n",
            "Closest match by euclidean distance: italy\n",
            "france - paris = italy - rome\n",
            "Closest match by cosine distance: italy\n",
            "france - paris = italy - rome\n",
            "Closest match by euclidean distance: rome\n",
            "paris - france = rome - italy\n",
            "Closest match by cosine distance: rome\n",
            "paris - france = rome - italy\n",
            "Closest match by euclidean distance: england\n",
            "france - french = england - english\n",
            "Closest match by cosine distance: england\n",
            "france - french = england - english\n",
            "Closest match by euclidean distance: china\n",
            "japan - japanese = china - chinese\n",
            "Closest match by cosine distance: china\n",
            "japan - japanese = china - chinese\n",
            "Closest match by euclidean distance: italy\n",
            "japan - japanese = italy - italian\n",
            "Closest match by cosine distance: italy\n",
            "japan - japanese = italy - italian\n",
            "Closest match by euclidean distance: australia\n",
            "japan - japanese = australia - australian\n",
            "Closest match by cosine distance: australia\n",
            "japan - japanese = australia - australian\n",
            "Closest match by euclidean distance: august\n",
            "december - november = august - june\n",
            "Closest match by cosine distance: august\n",
            "december - november = august - june\n",
            "** concat: False\n",
            "Closest match by euclidean distance: henry\n",
            "king - man = henry - woman\n",
            "Closest match by cosine distance: henry\n",
            "king - man = henry - woman\n",
            "Closest match by euclidean distance: england\n",
            "france - paris = england - london\n",
            "Closest match by cosine distance: england\n",
            "france - paris = england - london\n",
            "Closest match by euclidean distance: italy\n",
            "france - paris = italy - rome\n",
            "Closest match by cosine distance: italy\n",
            "france - paris = italy - rome\n",
            "Closest match by euclidean distance: rome\n",
            "paris - france = rome - italy\n",
            "Closest match by cosine distance: rome\n",
            "paris - france = rome - italy\n",
            "Closest match by euclidean distance: england\n",
            "france - french = england - english\n",
            "Closest match by cosine distance: england\n",
            "france - french = england - english\n",
            "Closest match by euclidean distance: china\n",
            "japan - japanese = china - chinese\n",
            "Closest match by cosine distance: china\n",
            "japan - japanese = china - chinese\n",
            "Closest match by euclidean distance: italy\n",
            "japan - japanese = italy - italian\n",
            "Closest match by cosine distance: italy\n",
            "japan - japanese = italy - italian\n",
            "Closest match by euclidean distance: australia\n",
            "japan - japanese = australia - australian\n",
            "Closest match by cosine distance: australia\n",
            "japan - japanese = australia - australian\n",
            "Closest match by euclidean distance: august\n",
            "december - november = august - june\n",
            "Closest match by cosine distance: august\n",
            "december - november = august - june\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}