{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVe_implement_from scratch_numpy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "65pyBf3KHeuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "#os.chdir('/content/drive/My Drive/Colab Notebooks/Lazy courses/NLP2/pretrained_data')\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/Lazy courses/NLP2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BwO3qv1H9am",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "from sklearn.utils import shuffle \n",
        "from utils import find_analogies\n",
        "\n",
        "from utils import get_wikipedia_data\n",
        "from brown import get_sentences_with_word2idx_limit_vocab, get_sentences_with_word2idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl916yuGUK_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ALS minimizes two loss functions alternatively; \n",
        "#It first holds user matrix fixed and runs gradient descent with item matrix; \n",
        "#then it holds item matrix fixed and runs gradient descent with user matrix\n",
        "#Using ALS whats the least number of files to get correct analogies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KGAC-chH9dP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Glove:\n",
        "    def __init__(self, D, V, context_sz):\n",
        "        self.D = D\n",
        "        self.V = V\n",
        "        self.context_sz = context_sz\n",
        "\n",
        "    def fit(self, sentences, cc_matrix=None, learning_rate=1e-4, reg=0.1, xmax=100, alpha=0.75, epochs=10, gd=False):\n",
        "        #build co-occurance matrix\n",
        "        t0 = datetime.now()\n",
        "        V = self.V\n",
        "        D = self.D\n",
        "\n",
        "        if not os.path.exists(cc_matrix):\n",
        "            X = np.zeros((V, V))\n",
        "            N = len(sentences)\n",
        "            print(\"Number of senetences to process\", N)\n",
        "            it = 0\n",
        "\n",
        "            for sentence in sentences:\n",
        "                it += 1\n",
        "                if it % 10000 == 0:\n",
        "                    print(f\"Processed {it} / {N}\")\n",
        "                n =len(sentence)\n",
        "                for i in range(n):\n",
        "                    #i points to which element of the sentence we are looking at\n",
        "                    wi = sentence[i]\n",
        "\n",
        "                    start = max(0, i - self.context_sz) \n",
        "                    end = min(n, i + self.context_sz)\n",
        "\n",
        "                    #we can either choose only one side as context, or both\n",
        "                    # Here we are choosing both\n",
        "                    #we have to make sure 'start' and 'end' tokens are part of\n",
        "                    #some context otherwise their f(X) will be 0 \n",
        "                    #(denominator in bias update)\n",
        "\n",
        "                    if i - self.context_sz < 0:\n",
        "                        points = 1.0 / (i+1)\n",
        "                        X[wi,0] += points\n",
        "                        X[0,wi] += points\n",
        "\n",
        "                    if i + self.context_sz > 0:\n",
        "                        points = 1.0 / (n-i)\n",
        "                        X[wi,1]  += points\n",
        "                        X[1,wi] += points\n",
        "\n",
        "                    #left side \n",
        "                    for j in range(start, i):\n",
        "                        wj = sentence[j]\n",
        "                        points = 1.0 / (i - j) #this is +ve\n",
        "                        X[wi, wj] += points\n",
        "                        X[wj, wi] += points\n",
        "                    #right side    \n",
        "                    for j in range(i+1, end):\n",
        "                        wj = sentence[j]\n",
        "                        points = 1.0 /(j - i) #this is +ve\n",
        "                        X[wi, wj] += points\n",
        "                        X[wj, wi] += points\n",
        "\n",
        "            np.save(cc_matrix, X)\n",
        "        else:\n",
        "            X = np.load(cc_matrix)\n",
        "\n",
        "        print(\"max in X\", X.max())\n",
        "\n",
        "        #weight normalization\n",
        "        fX = np.zeros((V, V))\n",
        "        fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n",
        "        fX[X >= xmax] = 1\n",
        "\n",
        "        print(\"max in f(X)\", fX.max())\n",
        "\n",
        "        #target\n",
        "        logX = np.log(X + 1)\n",
        "\n",
        "        print(\"max in log(X):\", logX.max())\n",
        "\n",
        "        print(\"Building co-occurance matrix:\", (datetime.now()- t0))\n",
        "\n",
        "        #initialize weights\n",
        "        W = np.random.randn(V, D) / np.sqrt(V + D)\n",
        "        b = np.zeros(V)\n",
        "        U = np.random.randn(V,D) / np.sqrt(V + D)\n",
        "        c = np.zeros(V)\n",
        "        mu = logX.mean()\n",
        "\n",
        "        costs = []\n",
        "        sentences_indexes = range(len(sentences))\n",
        "        for epoch in range(epochs):\n",
        "            delta = W.dot(U.T) + b.reshape(V, 1) + c.reshape(1, V) + mu - logX\n",
        "            cost = ( fX * delta * delta).sum()\n",
        "            costs.append(cost)\n",
        "            print(\"Epoch:\", epoch, \"cost:\", cost)\n",
        "\n",
        "            if gd:\n",
        "                #gradient descent method\n",
        "\n",
        "                #update W\n",
        "                for i in range(V):\n",
        "                    # W[i] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logX[i,j])*U[j]\n",
        "                    W -= learning_rate * (fX[i,:] * delta[i,:]).dot(U)\n",
        "                W -= learning_rate*reg*W\n",
        "\n",
        "                #update b\n",
        "                for i in range(V):\n",
        "                    # b[i] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logX[i,j])\n",
        "                    b[i] -= learning_rate * fX[i,:].dot(delta[i,:])\n",
        "                #b -= learning_rate * reg* W\n",
        "\n",
        "                #update U\n",
        "                for j in range(V):\n",
        "                    # U[j] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logX[i,j])*W[i]\n",
        "                    U[j]  -= learning_rate * (fX[:, j] * delta[:,j]).dot(W)\n",
        "                U -= learning_rate * reg * U \n",
        "\n",
        "                #update c\n",
        "                for j in range(V):\n",
        "                    #  c[j] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logX[i,j])\n",
        "                    c[j] -= learning_rate * fX[:,j].dot(delta[:, j])\n",
        "                 # c -= learning_rate*reg*c\n",
        "\n",
        "            else:\n",
        "                #ALS method\n",
        "\n",
        "                #update W\n",
        "                \n",
        "                t0 = datetime.now()\n",
        "\n",
        "                for i in range(V):\n",
        "                    #matrix = reg * np.eye(D) + (fX[i,:] * U.T).dot(U)\n",
        "                    matrix = reg*np.eye(D) + (fX[i,:]*U.T).dot(U)\n",
        "                    #vector = (fX[i,:] * logX[i,:] * U.T).dot(U)\n",
        "                    vector = (fX[i,:]*(logX[i,:] - b[i] - c - mu)).dot(U)\n",
        "                    W[i] = np.linalg.solve(matrix, vector)\n",
        "\n",
        "                    #matrix = reg*np.eye(D) + (fX[i,:]*U.T).dot(U)\n",
        "                    # assert(np.abs(matrix - matrix2).sum() < 1e-5)\n",
        "                    #vector = (fX[i,:]*(logX[i,:] - b[i] - c - mu)).dot(U)\n",
        "\n",
        "                print(\"ALS method took:\", datetime.now() - t0)\n",
        "\n",
        "                #update b\n",
        "                for i in range(V):\n",
        "                    denominator = fX[i, :].sum() + reg\n",
        "                    numerator = fX[i,:].dot(logX[i,:] - W[i].dot(U.T) - c -mu)\n",
        "                    b[i] = numerator / denominator\n",
        "\n",
        "                #update U\n",
        "                for j in range(V):\n",
        "                    matrix = reg* np.eye(D) + (fX[:,j] * W.T).dot(W)\n",
        "                    vector = (fX[:, j] * (logX[:,j] - b - c[j] - mu))\n",
        "\n",
        "                #update c\n",
        "                for j in range(V):\n",
        "                    denominator = fX[:, j].sum() + reg\n",
        "                    numerator = fX[:, j].dot(logX[:,j]- W.dot(U[j]) - b - mu)\n",
        "                    c[j] = numerator / denominator\n",
        "\n",
        "\n",
        "        self.W = W\n",
        "        self.U = U\n",
        "\n",
        "        plt.plot(costs)\n",
        "        plt.show()\n",
        "\n",
        "    \n",
        "    def save(self, fn):\n",
        "        #function word analogies expect a (V,D) matrix and a (D,V) matrix\n",
        "        arrays = [self.W, self.U.T]\n",
        "        np.savez(fn, *arrays)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_kX-Z-5H9fU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(we_file, w2i_file, use_brown=True, n_files=100):\n",
        "    if use_brown:\n",
        "        cc_matrix = \"cc_matrix_brown.npy\"\n",
        "    else:\n",
        "        cc_matrix = \"cc_matrix_%s.npy\" % n_files\n",
        "\n",
        "    #same old way of checking if we need to re-load the raw data or not\n",
        "    #remember, only the co-occurence matrix is needed for training\n",
        "\n",
        "    if os.path.exists(cc_matrix):\n",
        "        with open(w2i) as f:\n",
        "            word2idx = json.load(f)\n",
        "        sentences = [] #dummy list, we wont actually use it\n",
        "\n",
        "    else:\n",
        "        if use_brown:\n",
        "            keep_words = set([\n",
        "                'king', 'man', 'woman',\n",
        "                'france', 'paris', 'london', 'rome', 'italy', 'britain', 'england',\n",
        "                'french', 'english', 'japan', 'japanese', 'chinese', 'italian',\n",
        "                'australia', 'australian', 'december', 'november', 'june',\n",
        "                'january', 'february', 'march', 'april', 'may', 'july', 'august',\n",
        "                'september', 'october',\n",
        "            ])\n",
        "\n",
        "            sentences, word2idx = get_sentences_with_word2idx_limit_vocab(n_vocab=5000, keep_words=keep_words)\n",
        "\n",
        "        else:\n",
        "            sentences, word2idx = get_wikipedia_data(n_files = n_files, n_vocab = 2000)\n",
        "\n",
        "        with open(w2i_file, 'w') as f:\n",
        "            json.dump(word2idx, f)\n",
        "\n",
        "    V = len(word2idx)\n",
        "    model = Glove(100, V, 10)\n",
        "\n",
        "    #alternating least square method\n",
        "    model.fit(sentences, cc_matrix=cc_matrix, epochs=20)\n",
        "\n",
        "    #gradient descent method\n",
        "    #model.fit(sentences, cc_matrix=cc_matrix. learning_rate=5e-4, reg=0.1epochs=500, gd=True)\n",
        "    \n",
        "    model.save(we_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTLltH1JH9hp",
        "colab_type": "code",
        "outputId": "b2f997ef-0ea6-45a8-aac0-63941f63b845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    we = 'glove_model_50.npz'\n",
        "    w2i = 'glove_word2idx_50.json'\n",
        "    # we = 'glove_model_brown.npz'\n",
        "    # w2i = 'glove_word2idx_brown.json'\n",
        "\n",
        "    main(we, w2i, use_brown=False)\n",
        "\n",
        "    #load back embeddings\n",
        "    npz = np.load(we)\n",
        "    W1 = npz['arr_0']\n",
        "    W2 = npz['arr_1']\n",
        "\n",
        "    with open(w2i) as f:\n",
        "        word2idx = json.load(f)\n",
        "        idx2word = {i:w for w,i in word2idx.items()}\n",
        "\n",
        "    for concat in (True, False):\n",
        "        print(\"** concat:\", concat)\n",
        "\n",
        "        if concat:\n",
        "            We = np.hstack([W1, W2.T])\n",
        "        else:\n",
        "            We = (W1 + W2.T) / 2\n",
        "\n",
        "        \n",
        "        find_analogies('king', 'man', 'woman', We, word2idx, idx2word)\n",
        "        find_analogies('france', 'paris', 'london', We, word2idx, idx2word)\n",
        "        find_analogies('france', 'paris', 'rome', We, word2idx, idx2word)\n",
        "        find_analogies('paris', 'france', 'italy', We, word2idx, idx2word)\n",
        "        find_analogies('france', 'french', 'english', We, word2idx, idx2word)\n",
        "        find_analogies('japan', 'japanese', 'chinese', We, word2idx, idx2word)\n",
        "        find_analogies('japan', 'japanese', 'italian', We, word2idx, idx2word)\n",
        "        find_analogies('japan', 'japanese', 'australian', We, word2idx, idx2word)\n",
        "        find_analogies('december', 'november', 'june', We, word2idx, idx2word)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max in X 75446433.19263569\n",
            "max in f(X) 1.0\n",
            "max in log(X): 18.138933481526337\n",
            "Building co-occurance matrix: 0:00:00.613202\n",
            "Epoch: 0 cost: 5754260.947194482\n",
            "ALS method took: 0:00:05.800280\n",
            "Epoch: 1 cost: 851242.7041293962\n",
            "ALS method took: 0:00:05.758970\n",
            "Epoch: 2 cost: 754680.8432139151\n",
            "ALS method took: 0:00:05.721754\n",
            "Epoch: 3 cost: 746705.265816185\n",
            "ALS method took: 0:00:05.722831\n",
            "Epoch: 4 cost: 745562.1915970909\n",
            "ALS method took: 0:00:05.725532\n",
            "Epoch: 5 cost: 745232.973645697\n",
            "ALS method took: 0:00:05.716624\n",
            "Epoch: 6 cost: 745089.2347541997\n",
            "ALS method took: 0:00:05.778286\n",
            "Epoch: 7 cost: 745018.9500983905\n",
            "ALS method took: 0:00:05.738253\n",
            "Epoch: 8 cost: 744984.6045371497\n",
            "ALS method took: 0:00:05.718889\n",
            "Epoch: 9 cost: 744968.6117581232\n",
            "ALS method took: 0:00:05.725586\n",
            "Epoch: 10 cost: 744961.9516388458\n",
            "ALS method took: 0:00:05.734137\n",
            "Epoch: 11 cost: 744959.9181045637\n",
            "ALS method took: 0:00:05.777116\n",
            "Epoch: 12 cost: 744960.0612955366\n",
            "ALS method took: 0:00:05.765167\n",
            "Epoch: 13 cost: 744961.1224850805\n",
            "ALS method took: 0:00:05.692614\n",
            "Epoch: 14 cost: 744962.4722106698\n",
            "ALS method took: 0:00:05.747592\n",
            "Epoch: 15 cost: 744963.8115368538\n",
            "ALS method took: 0:00:05.714953\n",
            "Epoch: 16 cost: 744965.0127177754\n",
            "ALS method took: 0:00:07.672034\n",
            "Epoch: 17 cost: 744966.0343046633\n",
            "ALS method took: 0:00:05.772821\n",
            "Epoch: 18 cost: 744966.8762113057\n",
            "ALS method took: 0:00:05.751337\n",
            "Epoch: 19 cost: 744967.556269101\n",
            "ALS method took: 0:00:05.742627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "** concat: True\n",
            "Closest match by euclidean distance: ii\n",
            "king - man = ii - woman\n",
            "Closest match by cosine distance: louis\n",
            "king - man = louis - woman\n",
            "Closest match by euclidean distance: kingdom\n",
            "france - paris = kingdom - london\n",
            "Closest match by cosine distance: kingdom\n",
            "france - paris = kingdom - london\n",
            "Closest match by euclidean distance: spain\n",
            "france - paris = spain - rome\n",
            "Closest match by cosine distance: spain\n",
            "france - paris = spain - rome\n",
            "Closest match by euclidean distance: london\n",
            "paris - france = london - italy\n",
            "Closest match by cosine distance: london\n",
            "paris - france = london - italy\n",
            "Closest match by euclidean distance: england\n",
            "france - french = england - english\n",
            "Closest match by cosine distance: england\n",
            "france - french = england - english\n",
            "Closest match by euclidean distance: india\n",
            "japan - japanese = india - chinese\n",
            "Closest match by cosine distance: india\n",
            "japan - japanese = india - chinese\n",
            "Closest match by euclidean distance: india\n",
            "japan - japanese = india - italian\n",
            "Closest match by cosine distance: india\n",
            "japan - japanese = india - italian\n",
            "Closest match by euclidean distance: australia\n",
            "japan - japanese = australia - australian\n",
            "Closest match by cosine distance: australia\n",
            "japan - japanese = australia - australian\n",
            "Closest match by euclidean distance: august\n",
            "december - november = august - june\n",
            "Closest match by cosine distance: august\n",
            "december - november = august - june\n",
            "** concat: False\n",
            "Closest match by euclidean distance: ii\n",
            "king - man = ii - woman\n",
            "Closest match by cosine distance: henry\n",
            "king - man = henry - woman\n",
            "Closest match by euclidean distance: kingdom\n",
            "france - paris = kingdom - london\n",
            "Closest match by cosine distance: kingdom\n",
            "france - paris = kingdom - london\n",
            "Closest match by euclidean distance: spain\n",
            "france - paris = spain - rome\n",
            "Closest match by cosine distance: spain\n",
            "france - paris = spain - rome\n",
            "Closest match by euclidean distance: london\n",
            "paris - france = london - italy\n",
            "Closest match by cosine distance: london\n",
            "paris - france = london - italy\n",
            "Closest match by euclidean distance: england\n",
            "france - french = england - english\n",
            "Closest match by cosine distance: england\n",
            "france - french = england - english\n",
            "Closest match by euclidean distance: india\n",
            "japan - japanese = india - chinese\n",
            "Closest match by cosine distance: india\n",
            "japan - japanese = india - chinese\n",
            "Closest match by euclidean distance: india\n",
            "japan - japanese = india - italian\n",
            "Closest match by cosine distance: italy\n",
            "japan - japanese = italy - italian\n",
            "Closest match by euclidean distance: australia\n",
            "japan - japanese = australia - australian\n",
            "Closest match by cosine distance: australia\n",
            "japan - japanese = australia - australian\n",
            "Closest match by euclidean distance: august\n",
            "december - november = august - june\n",
            "Closest match by cosine distance: august\n",
            "december - november = august - june\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smiS9cM8VIU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR4IQLeNVIYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzx_0CFOVIb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kgS1d5YVIem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGtdd4QgVIhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GceD99LFVIke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}