{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hidden_markov_model_from_scratch_for_discrete_observations.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "18-N1eTc-ti8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os ; os.chdir('/content/drive/My Drive/Colab Notebooks/Lazy courses/Markov Models')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwVEhC1c_QbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72gi3STp_Qdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_normalized(d1, d2):\n",
        "    x = np.random.random((d1,d2))\n",
        "    return x / x.sum(axis=1, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMUJ-UxD_Qid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HMM:\n",
        "    def __init__(self, M):\n",
        "        self.M = M #number of hidden states\n",
        "\n",
        "    def fit(self, X, max_iter=30):\n",
        "        t0 = datetime.now()\n",
        "        np.random.seed(123)\n",
        "        #train the HMM model using Baum-Welch algorithm\n",
        "        #a specific instance of the expectation-maximization algorithm\n",
        "\n",
        "        #determine V, the vocabulary size\n",
        "        #assume observables are already integers from 0.. V-1\n",
        "        #X is a jagged array of observed sequences\n",
        "        V = max(max(x) for x in X) + 1\n",
        "        N = len(X)\n",
        "        self.pi = np.ones(self.M) / self.M #initial state distribution\n",
        "        self.A = random_normalized(self.M, self.M) #state transition matrix\n",
        "        self.B = random_normalized(self.M, V) #output distribution\n",
        "\n",
        "        print(\"initial A:\", self.A)\n",
        "        print(\"initial B:\", self.B)\n",
        "\n",
        "        costs = []\n",
        "        for it in range(max_iter):\n",
        "            if it % 10 == 0:\n",
        "                print(\"it\", it)\n",
        "            alphas = []\n",
        "            betas = []\n",
        "            P = np.zeros(N)\n",
        "            for n in range(N):\n",
        "                x = X[n]\n",
        "                T = len(x)\n",
        "\n",
        "                #calculate alpha\n",
        "                alpha = np.zeros((T, self.M))\n",
        "                alpha[0] = self.pi * self.B[:, x[0]] #calculate the 1st element\n",
        "                for t in range(1, T):\n",
        "                    tmp1 = alpha[t-1].dot(self.A) * self.B[:, x[t]]\n",
        "                    alpha[t] = tmp1\n",
        "                P[n] = alpha[-1].sum()\n",
        "                alphas.append(alpha)\n",
        "\n",
        "                #calculate beta\n",
        "                beta = np.zeros((T, self.M))\n",
        "                beta[-1] = 1\n",
        "                for t in range(T-2, -1, -1):\n",
        "                    beta[t] = self.A.dot(self.B[:, x[t+1]] * beta[t+1])\n",
        "                betas.append(beta)\n",
        "\n",
        "            assert(np.all(P > 0))\n",
        "            cost = np.sum(np.log(P))\n",
        "            costs.append(cost)\n",
        "\n",
        "            #now re-estimate pi, A ,B\n",
        "            self.pi = np.sum((alphas[n][0] * betas[n][0])/P[n] for n in range(N)) / N\n",
        "\n",
        "            den1 = np.zeros((self.M, 1))\n",
        "            den2 = np.zeros((self.M, 1))\n",
        "            a_num = 0\n",
        "            b_num = 0\n",
        "            for n in range(N):\n",
        "                x = X[n]\n",
        "                T = len(x)\n",
        "                den1 += (alphas[n][:-1] * betas[n][:-1]).sum(axis=0, keepdims=True).T / P[n]\n",
        "                den2 += (alphas[n] * betas[n]).sum(axis=0, keepdims=True).T / P[n]\n",
        "\n",
        "                #numerator for A\n",
        "                a_num_n = np.zeros((self.M, self.M))\n",
        "                for i in range(self.M):\n",
        "                    for j in range(self.M):\n",
        "                        for t in range(T-1):\n",
        "                            a_num_n[i,j] += alphas[n][t,i] * self.A[i,j] * self.B[j, x[t+1]] * betas[n][t+1, j]\n",
        "                a_num += a_num_n / P[n]\n",
        "\n",
        "                #numerator for B\n",
        "                b_num_n2 = np.zeros((self.M, V))\n",
        "                for i in range(self.M):\n",
        "                    for t in range(T):\n",
        "                        b_num_n2[i, x[t]] += alphas[n][t,i] * betas[n][t,i]\n",
        "                b_num += b_num_n2 / P[n]\n",
        "\n",
        "            self.A = a_num / den1\n",
        "            self.B = b_num / den2\n",
        "\n",
        "        print(\"A:\", self.A)\n",
        "        print(\"B:\", self.B)\n",
        "        print(\"pi:\", self.pi)\n",
        "\n",
        "        print(\"Fit duration:\", (datetime.now() - t0))\n",
        "\n",
        "        plt.plot(costs)\n",
        "        plt.show()\n",
        "\n",
        "    def likelihood(self, x):\n",
        "        #return log P(x | model)\n",
        "        #using the forward part of the forward-backward algorithm\n",
        "        T = len(x)\n",
        "        alpha = np.zeros((T, self.M))\n",
        "        alpha[0] = self.pi * self.B[:, x[0]]\n",
        "        for t in range(1, T):\n",
        "            alpha[t] = alpha[t-1].dot(self.A) * self.B[:,x[t]]\n",
        "        return alpha[-1].sum()\n",
        "\n",
        "    def likelihood_multi(self, X):\n",
        "        return np.array([self.likelihood(x) for x in X])\n",
        "\n",
        "    def log_likelihood_multi(self, X):\n",
        "        return np.log(self.likelihood_multi(X))\n",
        "\n",
        "    def get_state_sequence(self, x):\n",
        "        #returns the most likely state sequence given observed sequence x\n",
        "        #using the viterbi algorithm\n",
        "        T = len(x)\n",
        "        delta = np.zeros((T, self.M))\n",
        "        psi = np.zeros((T, self.M))\n",
        "        delta[0] = self.pi * self.B[:, x[0]]\n",
        "        for t in range(1,T):\n",
        "            for j in range(self.M):\n",
        "                delta[t, j] = np.max(delta[t-1] * self.A[:,j]) * self.B[j, x[t]]\n",
        "                psi[t, j] = np.argmax(delta[t-1] * self.A[:, j])\n",
        "\n",
        "        #backtrack\n",
        "        states = np.zeros(T, dtype=np.int32)\n",
        "        states[T-1] = np.argmax(delta[T-1])\n",
        "        for t in range(T-2, -1, -1):\n",
        "            states[t] = psi[t+1, states[t+1]]\n",
        "\n",
        "        return states    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hjmPl3p_QlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_coin():\n",
        "    X = []\n",
        "    for line in open('coin_data.txt'):\n",
        "        # 1 for H, 0 for T\n",
        "        x = [1 if e == 'H' else 0 for e in line.rstrip()]\n",
        "        X.append(x)\n",
        "\n",
        "    hmm = HMM(2)\n",
        "    hmm.fit(X)\n",
        "    L = hmm.log_likelihood_multi(X).sum()\n",
        "    print(\"LL with fitted params:\", L)\n",
        "\n",
        "    #try some values \n",
        "    hmm.pi = np.array([0.5, 0.5])\n",
        "    hmm.A = np.array([[0.1, 0.9], [0.8, 0.2]])\n",
        "    hmm.B = np.array([[0.6, 0.4], [0.3, 0.7]])\n",
        "    L = hmm.log_likelihood_multi(X).sum()\n",
        "    print(\"LL with True params:\", L)\n",
        "\n",
        "    #try viterbi\n",
        "    print(\"Best state sequence for:\", X[0], '\\n')\n",
        "    print(hmm.get_state_sequence(X[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZggAFoV_Qra",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "outputId": "bb2df6f4-c71e-4848-90dc-083ebd42ab5b"
      },
      "source": [
        "if __name__ =='__main__':\n",
        "    fit_coin()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial A: [[0.7087962  0.2912038 ]\n",
            " [0.29152056 0.70847944]]\n",
            "initial B: [[0.62969057 0.37030943]\n",
            " [0.58883752 0.41116248]]\n",
            "it 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it 10\n",
            "it 20\n",
            "A: [[0.70386662 0.29613338]\n",
            " [0.28712763 0.71287237]]\n",
            "B: [[0.54419694 0.45580306]\n",
            " [0.53723247 0.46276753]]\n",
            "pi: [0.50695647 0.49304353]\n",
            "Fit duration: 0:00:00.704442\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LL with fitted params: -1034.7557547352071\n",
            "LL with True params: -1059.7229160265022\n",
            "Best state sequence for: [0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1] \n",
            "\n",
            "[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo-vnszn0FhW",
        "colab_type": "text"
      },
      "source": [
        "From the plot we can see that expectation-maximization converges very fast.\n",
        "We also see that fitted log-likelihood is better than true log-likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipz205um_Qv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0BwFZU7_Qyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVQ2kRV7_QuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTalCC9i_QpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvdPtYlP_QnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}