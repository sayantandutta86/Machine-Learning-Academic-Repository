{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_implement_from_scratch_using_numpy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF73vYs00jSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os;os.chdir('/content/drive/My Drive/Colab Notebooks/Lazy courses/NLP2/pretrained_data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vkihQELGGfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import expit as sigmoid\n",
        "from sklearn.utils import shuffle\n",
        "from datetime import datetime\n",
        "\n",
        "from scipy.spatial.distance import cosine as cos_dist\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import string\n",
        "import pdb\n",
        "\n",
        "# import os; os.chdir('/content/drive/My Drive/Colab Notebooks/Lazy courses/NLP2')\n",
        "# from brown import get_sentences_with_word2idx_limit_vocab as get_brown\n",
        "savedir = '/content/drive/My Drive/Colab Notebooks/Lazy courses/NLP2/pretrained_data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhyfZbeYGGhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punctuation(s):\n",
        "    return s.translate(str.maketrans('','', string.punctuation))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJyPDnT4GGmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHkLr1d-GGoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_wiki():\n",
        "\n",
        "    V = 20000\n",
        "\n",
        "    wiki_data_location = '/content/drive/My Drive/Colab Notebooks/Lazy courses/NLP2/wiki_data'\n",
        "    os.chdir(wiki_data_location)\n",
        "    \n",
        "    files = [f for f in os.listdir(wiki_data_location) if f.startswith('enwiki') and f.endswith('txt')]\n",
        "\n",
        "    #pdb.set_trace()\n",
        "\n",
        "    #files = glob('enwiki*.txt')\n",
        "    all_word_counts = {}\n",
        "\n",
        "    for f in files:\n",
        "        for line in open(f):\n",
        "            if line and line[0] not in '[*-|=\\{\\}':\n",
        "                s = remove_punctuation(line).lower().split()\n",
        "                if len(s) > 1:\n",
        "                    for word in s:\n",
        "                        if word not in all_word_counts:\n",
        "                            all_word_counts[word] = 0\n",
        "                        all_word_counts[word] += 1\n",
        "    print(\"Finished counting..\")\n",
        "\n",
        "    V = min(V, len(all_word_counts))\n",
        "    all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    top_words = [w for w, count in all_word_counts[:V-1]] + ['<UNK>']\n",
        "    word2idx = {w:i for i,w in enumerate(top_words)}\n",
        "    unk = word2idx['<UNK>']\n",
        "\n",
        "    sents = []\n",
        "    for f in files:\n",
        "        for line in open(f):\n",
        "            if line and line[0] not in '[*-|=\\{\\}':\n",
        "                s = remove_punctuation(line).lower().split()\n",
        "                if len(s) > 1:\n",
        "                    #if a word is not nearby another word, \n",
        "                    #there won't be any context, and hence nothing to train!\n",
        "                    sent = [word2idx[w] if w in word2idx else unk for w in s]\n",
        "                    sents.append(sent)\n",
        "\n",
        "    return sents, word2idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQjnql3goV6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_negative_sampling_distribution(sentences, vocab_size):\n",
        "    #Pn(w) = probability of word occuring\n",
        "    #we would like to sample the negative samples\n",
        "    #such that words that occur more often \n",
        "    #should be sampled more often\n",
        "\n",
        "    word_freq = np.zeros(vocab_size)\n",
        "    word_count = sum(len(sentence) for sentence in sentences)\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            word_freq[word] += 1\n",
        "\n",
        "    #smoothen \n",
        "    p_neg = word_freq**0.75\n",
        "\n",
        "    #normalize\n",
        "    p_neg = p_neg / p_neg.sum()\n",
        "\n",
        "    # ####### DEBUG\n",
        "    # print(\"###\")\n",
        "    # print(vocab_size)\n",
        "    # #print(np.all(p_neg > 0))\n",
        "    # print(\"###\")\n",
        "    # #######\n",
        "\n",
        "    assert(np.all(p_neg > 0))\n",
        "    return p_neg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chx3cWEvzXCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_context(pos, sentence, window_size):\n",
        "    #input:\n",
        "    #a sentence of the form: x x x x c c c pos c c c x x x x \n",
        "    #output:\n",
        "    #the context word indices: c c c c c c\n",
        "\n",
        "    start = max(0, pos - window_size)\n",
        "    end_ = min(len(sentence), pos + window_size) \n",
        "\n",
        "    context = []\n",
        "    for ctx_pos, ctx_word_idx in enumerate(sentence[start:end_], start=start):\n",
        "        if ctx_pos != pos:\n",
        "            #don't include the input word itself as a target\n",
        "            context.append(ctx_word_idx)\n",
        "    \n",
        "    return context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-sLACnna_4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sgd(input_, targets, label, learning_rate, W, V):\n",
        "    # W[input_] shape: D\n",
        "    # V[;, targets] shape DxN\n",
        "    #activation shape: N\n",
        "    #print(\"input_\", input_, \"targets\", targets)\n",
        "    activation = W[input_].dot(V[:, targets])\n",
        "    prob = sigmoid(activation)\n",
        "\n",
        "    #gradients\n",
        "    gV = np.outer(W[input_], prob - label) #shape--> DxN\n",
        "    gW = np.sum((prob - label) * V[:, targets], axis=1) #shape--> D\n",
        "\n",
        "    V[:, targets] -= learning_rate * gV #shape--> D x N\n",
        "    W[input_] -= learning_rate * gW #shape--> D\n",
        "\n",
        "    #return cost --> binary cross-entropy\n",
        "    cost = label * np.log(prob + 1e-10) + (1 - label) * np.log(1 - prob + 1e-10)\n",
        "    return cost.sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PJZKG2IGGsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(savedir):\n",
        "    #get the data\n",
        "    sentences, word2idx = get_wiki()\n",
        "\n",
        "    #debug\n",
        "    #pdb.set_trace()\n",
        "\n",
        "\n",
        "    #number of unique words\n",
        "    vocab_size = len(word2idx)\n",
        "\n",
        "    #config\n",
        "    window_size = 5\n",
        "    learning_rate = 0.025\n",
        "    final_learning_rate = 0.0001\n",
        "    num_negatives = 5 # number of negatives samples to draw per input word\n",
        "    epochs = 20\n",
        "    D = 50 # word embedding size \n",
        "\n",
        "    #learning rate decay\n",
        "    learning_rate_delta = (learning_rate -final_learning_rate)/epochs\n",
        "\n",
        "    #params \n",
        "    W = np.random.randn(vocab_size, D) #input layer to hidden layer\n",
        "    V = np.random.randn(D, vocab_size) #hidden to output layer\n",
        "\n",
        "    #distribution for drawing negative samples\n",
        "    p_neg = get_negative_sampling_distribution(sentences, vocab_size)\n",
        "\n",
        "    #save the costs to plot them per iteration\n",
        "    costs = []\n",
        "\n",
        "    #number of total words in corpus\n",
        "    total_words = sum(len(sentence) for sentence in sentences)\n",
        "    print(\"Total number of words in corpus:\", total_words)\n",
        "\n",
        "    #for subsampling each sentence\n",
        "    threshold = 1e-5\n",
        "    p_drop = 1 - np.sqrt(threshold / p_neg)\n",
        "\n",
        "    #train the model\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        #shuffle the sentence order\n",
        "        np.random.shuffle(sentences)\n",
        "\n",
        "        #accumulate the cost\n",
        "        cost = 0 \n",
        "        counter = 0\n",
        "        t0 = datetime.now()\n",
        "\n",
        "        for sentence in sentences:\n",
        "            #keep only certain words based on p_neg\n",
        "            sentence = [w for w in sentence if np.random.random() < (1-p_drop[w])]\n",
        "\n",
        "            if len(sentence) < 2:\n",
        "                continue\n",
        "            \n",
        "            #randomly order words so we don't always \n",
        "            #see samples in the same order\n",
        "            randomly_ordered_positions = np.random.choice(\n",
        "                len(sentence), size=len(sentence), replace=False,\n",
        "            )\n",
        "\n",
        "            for pos in randomly_ordered_positions:\n",
        "                #the middle word\n",
        "                word = sentence[pos]\n",
        "\n",
        "                #get the positive context words and negative samples\n",
        "                context_words = get_context(pos, sentence, window_size)\n",
        "                neg_word = np.random.choice(vocab_size, p=p_neg)\n",
        "                targets = np.array(context_words)\n",
        "\n",
        "                #do one iteration of stochastic gradient descent\n",
        "                c = sgd(word, targets, 1, learning_rate, W, V)\n",
        "                cost += c\n",
        "                c = sgd(neg_word, targets, 0, learning_rate, W, V)\n",
        "                cost += c\n",
        "\n",
        "            counter += 1\n",
        "\n",
        "            if counter % 5000 == 0:\n",
        "                sys.stdout.write(f\"Processed {counter} / {len(sentences)} \\n\")\n",
        "                sys.stdout.flush()\n",
        "            \n",
        "        dt = datetime.now() - t0\n",
        "        print('epoch complete:', epoch, \"cost:\", cost, \"dt:\", dt)\n",
        "\n",
        "        #save the cost\n",
        "        costs.append(cost)\n",
        "\n",
        "        #update the learning rate\n",
        "        learning_rate -= learning_rate_delta\n",
        "\n",
        "    #plot the cost per iteration\n",
        "    plt.plot(costs)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Costs')\n",
        "    plt.show()\n",
        "\n",
        "    #Save the model\n",
        "    with open(f'{savedir}/word2idx.json', 'w') as f:\n",
        "        json.dump(word2idx, f)\n",
        "\n",
        "    np.savez(f\"{savedir}/weights.npz\", W, V)\n",
        "\n",
        "    #return the model \n",
        "    return word2idx, W, V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOjZPFWOGGwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model():\n",
        "    with open(f\"{savedir}/word2idx.json\") as f:\n",
        "        word2idx = json.load(f)\n",
        "    npz = np.load(f'{savedir}/weights.npz')\n",
        "    W = npz['arr_0']\n",
        "    V = npz['arr_1']\n",
        "    return word2idx, W, V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGOHTrzCGGzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word, W):\n",
        "    V, D = W.shape\n",
        "\n",
        "    #pos2 not used in calculation, just printing the expected value\n",
        "    print(f\"testing {pos1} - {neg1} = {pos2} - {neg2}\")\n",
        "    for w in (pos1, neg1, pos2, neg2):\n",
        "        if w not in word2idx:\n",
        "            print(f\"Sorry {w} is not word2idx\")\n",
        "            return\n",
        "\n",
        "    p1 = W[word2idx[pos1]]\n",
        "    n1 = W[word2idx[neg1]]\n",
        "    p2 = W[word2idx[pos2]]\n",
        "    n2 = W[word2idx[neg2]]\n",
        "\n",
        "    vec = p1 - n1 + n2\n",
        "\n",
        "    distances = pairwise_distances(vec.reshape(1, D), W, metric='cosine').reshape(V)\n",
        "    idx = distances.argsort()[:10]\n",
        "\n",
        "    #pick one that's not p1, n1, or n2\n",
        "    best_idx = -1\n",
        "    keep_out = [wordidx[w] for w in (pos1, neg1, neg2)]\n",
        "    #print(\"Keep_out\", keep_out)\n",
        "    for i in idx:\n",
        "        if i not in keep_out:\n",
        "            best_idx = i\n",
        "            break\n",
        "    # print(\"best_idx\",best_idx)\n",
        "\n",
        "    print(f\"got: {pos1} - {neg1} = {idx2word[best_idx]} - {neg2}\")\n",
        "    print(\"closest 10:\")\n",
        "    for i in idx:\n",
        "        print(idx2word[i], distances[i], '\\n')\n",
        "\n",
        "    print(f\"Distance to {pos2}: {cos_dist(p2, vec)}\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7KREoyEGGkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(word2idx, W, V):\n",
        "    #there are multiple ways to get the \"final\" word embedding\n",
        "    #We = (W + V.T) / 2\n",
        "    #We = W\n",
        "    idx2word = {i:w for w, i in word2idx.items()}\n",
        "\n",
        "    for We in (W, (W + V.T) / 2):\n",
        "        print(\"*\" * 30)\n",
        "        \n",
        "        analogy('king', 'man', 'queen', 'woman', word2idx, idx2word, We)\n",
        "        analogy('king', 'prince', 'queen', 'princess', word2idx, idx2word, We)\n",
        "        analogy('miami', 'florida', 'dallas', 'texas', word2idx, idx2word, We)\n",
        "        analogy('einstein', 'scientist', 'picasso', 'painter', word2idx, idx2word, We)\n",
        "        analogy('japan', 'sushi', 'germany', 'bratwurst', word2idx, idx2word, We)\n",
        "        analogy('man', 'woman', 'he', 'she', word2idx, idx2word, We)\n",
        "        analogy('man', 'woman', 'uncle', 'aunt', word2idx, idx2word, We)\n",
        "        analogy('man', 'woman', 'brother', 'sister', word2idx, idx2word, We)\n",
        "        analogy('man', 'woman', 'husband', 'wife', word2idx, idx2word, We)\n",
        "        analogy('man', 'woman', 'actor', 'actress', word2idx, idx2word, We)\n",
        "        analogy('man', 'woman', 'father', 'mother', word2idx, idx2word, We)\n",
        "        analogy('heir', 'heiress', 'prince', 'princess', word2idx, idx2word, We)\n",
        "        analogy('nephew', 'niece', 'uncle', 'aunt', word2idx, idx2word, We)\n",
        "        analogy('france', 'paris', 'japan', 'tokyo', word2idx, idx2word, We)\n",
        "        analogy('france', 'paris', 'china', 'beijing', word2idx, idx2word, We)\n",
        "        analogy('february', 'january', 'december', 'november', word2idx, idx2word, We)\n",
        "        analogy('france', 'paris', 'germany', 'berlin', word2idx, idx2word, We)\n",
        "        analogy('week', 'day', 'year', 'month', word2idx, idx2word, We)\n",
        "        analogy('week', 'day', 'hour', 'minute', word2idx, idx2word, We)\n",
        "        analogy('france', 'paris', 'italy', 'rome', word2idx, idx2word, We)\n",
        "        analogy('paris', 'france', 'rome', 'italy', word2idx, idx2word, We)\n",
        "        analogy('france', 'french', 'england', 'english', word2idx, idx2word, We)\n",
        "        analogy('japan', 'japanese', 'china', 'chinese', word2idx, idx2word, We)\n",
        "        analogy('china', 'chinese', 'america', 'american', word2idx, idx2word, We)\n",
        "        analogy('japan', 'japanese', 'italy', 'italian', word2idx, idx2word, We)\n",
        "        analogy('japan', 'japanese', 'australia', 'australian', word2idx, idx2word, We)\n",
        "        analogy('walk', 'walking', 'swim', 'swimming', word2idx, idx2word, We)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiC_zBkatO7y",
        "colab_type": "code",
        "outputId": "f53cf244-5e7c-4330-b2a9-7c9dc8563dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    word2idx, W, V = train_model(savedir)\n",
        "    test_model(word2idx, W, V)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished counting..\n",
            "Total number of words in corpus: 86478677\n",
            "Processed 5000 / 1271558 \n",
            "Processed 10000 / 1271558 \n",
            "Processed 15000 / 1271558 \n",
            "Processed 20000 / 1271558 \n",
            "Processed 25000 / 1271558 \n",
            "Processed 30000 / 1271558 \n",
            "Processed 35000 / 1271558 \n",
            "Processed 40000 / 1271558 \n",
            "Processed 45000 / 1271558 \n",
            "Processed 50000 / 1271558 \n",
            "Processed 55000 / 1271558 \n",
            "Processed 60000 / 1271558 \n",
            "Processed 65000 / 1271558 \n",
            "Processed 70000 / 1271558 \n",
            "Processed 75000 / 1271558 \n",
            "Processed 80000 / 1271558 \n",
            "Processed 85000 / 1271558 \n",
            "Processed 90000 / 1271558 \n",
            "Processed 95000 / 1271558 \n",
            "Processed 100000 / 1271558 \n",
            "Processed 105000 / 1271558 \n",
            "Processed 110000 / 1271558 \n",
            "Processed 115000 / 1271558 \n",
            "Processed 120000 / 1271558 \n",
            "Processed 125000 / 1271558 \n",
            "Processed 130000 / 1271558 \n",
            "Processed 135000 / 1271558 \n",
            "Processed 140000 / 1271558 \n",
            "Processed 145000 / 1271558 \n",
            "Processed 150000 / 1271558 \n",
            "Processed 155000 / 1271558 \n",
            "Processed 160000 / 1271558 \n",
            "Processed 165000 / 1271558 \n",
            "Processed 170000 / 1271558 \n",
            "Processed 175000 / 1271558 \n",
            "Processed 180000 / 1271558 \n",
            "Processed 185000 / 1271558 \n",
            "Processed 190000 / 1271558 \n",
            "Processed 195000 / 1271558 \n",
            "Processed 200000 / 1271558 \n",
            "Processed 205000 / 1271558 \n",
            "Processed 210000 / 1271558 \n",
            "Processed 215000 / 1271558 \n",
            "Processed 220000 / 1271558 \n",
            "Processed 225000 / 1271558 \n",
            "Processed 230000 / 1271558 \n",
            "Processed 235000 / 1271558 \n",
            "Processed 240000 / 1271558 \n",
            "Processed 245000 / 1271558 \n",
            "Processed 250000 / 1271558 \n",
            "Processed 255000 / 1271558 \n",
            "Processed 260000 / 1271558 \n",
            "Processed 265000 / 1271558 \n",
            "Processed 270000 / 1271558 \n",
            "Processed 275000 / 1271558 \n",
            "Processed 280000 / 1271558 \n",
            "Processed 285000 / 1271558 \n",
            "Processed 290000 / 1271558 \n",
            "Processed 295000 / 1271558 \n",
            "Processed 300000 / 1271558 \n",
            "Processed 305000 / 1271558 \n",
            "Processed 310000 / 1271558 \n",
            "Processed 315000 / 1271558 \n",
            "Processed 320000 / 1271558 \n",
            "Processed 325000 / 1271558 \n",
            "Processed 330000 / 1271558 \n",
            "Processed 335000 / 1271558 \n",
            "Processed 340000 / 1271558 \n",
            "Processed 345000 / 1271558 \n",
            "Processed 350000 / 1271558 \n",
            "Processed 355000 / 1271558 \n",
            "Processed 360000 / 1271558 \n",
            "Processed 365000 / 1271558 \n",
            "Processed 370000 / 1271558 \n",
            "Processed 375000 / 1271558 \n",
            "Processed 380000 / 1271558 \n",
            "Processed 385000 / 1271558 \n",
            "Processed 390000 / 1271558 \n",
            "Processed 395000 / 1271558 \n",
            "Processed 400000 / 1271558 \n",
            "Processed 405000 / 1271558 \n",
            "Processed 410000 / 1271558 \n",
            "Processed 415000 / 1271558 \n",
            "Processed 420000 / 1271558 \n",
            "Processed 425000 / 1271558 \n",
            "Processed 430000 / 1271558 \n",
            "Processed 435000 / 1271558 \n",
            "Processed 440000 / 1271558 \n",
            "Processed 445000 / 1271558 \n",
            "Processed 450000 / 1271558 \n",
            "Processed 455000 / 1271558 \n",
            "Processed 460000 / 1271558 \n",
            "Processed 465000 / 1271558 \n",
            "Processed 470000 / 1271558 \n",
            "Processed 475000 / 1271558 \n",
            "Processed 480000 / 1271558 \n",
            "Processed 485000 / 1271558 \n",
            "Processed 490000 / 1271558 \n",
            "Processed 495000 / 1271558 \n",
            "Processed 500000 / 1271558 \n",
            "Processed 505000 / 1271558 \n",
            "Processed 510000 / 1271558 \n",
            "Processed 515000 / 1271558 \n",
            "Processed 520000 / 1271558 \n",
            "Processed 525000 / 1271558 \n",
            "Processed 530000 / 1271558 \n",
            "Processed 535000 / 1271558 \n",
            "Processed 540000 / 1271558 \n",
            "Processed 545000 / 1271558 \n",
            "Processed 550000 / 1271558 \n",
            "Processed 555000 / 1271558 \n",
            "Processed 560000 / 1271558 \n",
            "Processed 565000 / 1271558 \n",
            "Processed 570000 / 1271558 \n",
            "Processed 575000 / 1271558 \n",
            "Processed 580000 / 1271558 \n",
            "Processed 585000 / 1271558 \n",
            "Processed 590000 / 1271558 \n",
            "Processed 595000 / 1271558 \n",
            "Processed 600000 / 1271558 \n",
            "Processed 605000 / 1271558 \n",
            "Processed 610000 / 1271558 \n",
            "Processed 615000 / 1271558 \n",
            "Processed 620000 / 1271558 \n",
            "Processed 625000 / 1271558 \n",
            "Processed 630000 / 1271558 \n",
            "Processed 635000 / 1271558 \n",
            "Processed 640000 / 1271558 \n",
            "Processed 645000 / 1271558 \n",
            "Processed 650000 / 1271558 \n",
            "Processed 655000 / 1271558 \n",
            "Processed 660000 / 1271558 \n",
            "Processed 665000 / 1271558 \n",
            "Processed 670000 / 1271558 \n",
            "Processed 675000 / 1271558 \n",
            "Processed 680000 / 1271558 \n",
            "Processed 685000 / 1271558 \n",
            "Processed 690000 / 1271558 \n",
            "Processed 695000 / 1271558 \n",
            "Processed 700000 / 1271558 \n",
            "Processed 705000 / 1271558 \n",
            "Processed 710000 / 1271558 \n",
            "Processed 715000 / 1271558 \n",
            "Processed 720000 / 1271558 \n",
            "Processed 725000 / 1271558 \n",
            "Processed 730000 / 1271558 \n",
            "Processed 735000 / 1271558 \n",
            "Processed 740000 / 1271558 \n",
            "Processed 745000 / 1271558 \n",
            "Processed 750000 / 1271558 \n",
            "Processed 755000 / 1271558 \n",
            "Processed 760000 / 1271558 \n",
            "Processed 765000 / 1271558 \n",
            "Processed 770000 / 1271558 \n",
            "Processed 775000 / 1271558 \n",
            "Processed 780000 / 1271558 \n",
            "Processed 785000 / 1271558 \n",
            "Processed 790000 / 1271558 \n",
            "Processed 795000 / 1271558 \n",
            "Processed 800000 / 1271558 \n",
            "Processed 805000 / 1271558 \n",
            "Processed 810000 / 1271558 \n",
            "Processed 815000 / 1271558 \n",
            "Processed 820000 / 1271558 \n",
            "Processed 825000 / 1271558 \n",
            "Processed 830000 / 1271558 \n",
            "Processed 835000 / 1271558 \n",
            "Processed 840000 / 1271558 \n",
            "Processed 845000 / 1271558 \n",
            "Processed 850000 / 1271558 \n",
            "Processed 855000 / 1271558 \n",
            "Processed 860000 / 1271558 \n",
            "Processed 865000 / 1271558 \n",
            "Processed 870000 / 1271558 \n",
            "Processed 875000 / 1271558 \n",
            "Processed 880000 / 1271558 \n",
            "Processed 885000 / 1271558 \n",
            "Processed 890000 / 1271558 \n",
            "Processed 895000 / 1271558 \n",
            "Processed 900000 / 1271558 \n",
            "Processed 905000 / 1271558 \n",
            "Processed 910000 / 1271558 \n",
            "Processed 915000 / 1271558 \n",
            "Processed 920000 / 1271558 \n",
            "Processed 925000 / 1271558 \n",
            "Processed 930000 / 1271558 \n",
            "Processed 935000 / 1271558 \n",
            "Processed 940000 / 1271558 \n",
            "Processed 945000 / 1271558 \n",
            "Processed 950000 / 1271558 \n",
            "Processed 955000 / 1271558 \n",
            "Processed 960000 / 1271558 \n",
            "Processed 965000 / 1271558 \n",
            "Processed 970000 / 1271558 \n",
            "Processed 975000 / 1271558 \n",
            "Processed 980000 / 1271558 \n",
            "Processed 985000 / 1271558 \n",
            "Processed 990000 / 1271558 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQZDfG9Mb2lV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnxAdJz-b2oK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZscMRjO3b2uG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aofmv3Jpb2sF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}