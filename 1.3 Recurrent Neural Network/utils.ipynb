{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "utils.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "33vEZNoOQkGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os; os.chdir('/content/drive/My Drive/Colab Notebooks/Lazy courses/Recurrent Neural Network')\n",
        "\n",
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "import sys\n",
        "import operator\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from datetime import datetime\n",
        "\n",
        "def get_tags(s):\n",
        "    tuples = pos_tag(word_tokenize(s))\n",
        "    return [y for x, y in tuples]\n",
        "\n",
        "def init_weight(Mi, Mo):\n",
        "    return np.random.randn(Mi, Mo) / np.sqrt(Mi + Mo)\n",
        "\n",
        "def all_parity_pairs(nbit):\n",
        "    #total number of samples (Ntotal) will be a multiple of 100\n",
        "    N = 2**nbit\n",
        "    remainder = 100 - (N % 100)\n",
        "    Ntotal = N + remainder\n",
        "    X = np.zeros((Ntotal, nbit))\n",
        "    Y = np.zeros(Ntotal)\n",
        "    for ii in range(Ntotal):\n",
        "        i = ii % N\n",
        "        #now generate the ith sample \n",
        "        for j in range(nbit):\n",
        "            if i % (2**(j+1)) != 0:\n",
        "                i -= 2**j\n",
        "                X[ii,j] = 1\n",
        "        Y[ii] = X[ii].sum() % 2 \n",
        "    return X, Y\n",
        "\n",
        "def all_parity_pairs_with_sequence_labels(nbit):\n",
        "    X, Y = all_parity_pairs(nbit)\n",
        "    N, t = X.shape\n",
        "\n",
        "    #we want every time step to have label\n",
        "    Y_t = np.zeros(X.shape, dtype=np.int32)\n",
        "    for n in range(N):\n",
        "        ones_count = 0\n",
        "        for i in range(t):\n",
        "            if X[n, i] == 1:\n",
        "                ones_count += 1\n",
        "            if ones_count % 2 ==1:\n",
        "                Y_t[n,i] = 1\n",
        "\n",
        "    X = X.reshape(N, t, 1).astype(np.float32)\n",
        "    return X, Y_t\n",
        "\n",
        "def remove_punctuation(s):\n",
        "    return s.translate(str.maketrans('','',string.punctuation))\n",
        "\n",
        "def my_tokenizer(s):\n",
        "    s = remove_punctuation(s)\n",
        "    s = s.lower()\n",
        "    return s.split()\n",
        "\n",
        "def get_robert_frost():\n",
        "    word2idx = {'START': 0, 'END': 1}\n",
        "    current_idx = 2\n",
        "    sentences = []\n",
        "    for line in open('/content/drive/My Drive/Colab Notebooks/Lazy courses/NLP2/Markov Models/robert_frost.txt'):\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            tokens = remove_punctuation(line.lower()).split()\n",
        "            sentence = []\n",
        "            for t in tokens:\n",
        "                if t not in word2idx:\n",
        "                    word2idx[t] = current_idx\n",
        "                    current_idx += 1\n",
        "                idx = word2idx[t]\n",
        "                sentence.append(idx)\n",
        "            sentences.append(sentence)\n",
        "    return sentences, word2idx\n",
        "\n",
        "def get_wikipedia_data(n_files, n_vocab, by_paragraph=False):\n",
        "\n",
        "    wiki_data_location = '/content/drive/My Drive/Colab Notebooks/Lazy courses/NLP2/wiki_data'\n",
        "\n",
        "    input_files = [f for f in os.listdir(wiki_data_location) if f.startswith('enwiki') and f.endswith('txt')]\n",
        "\n",
        "    if len(input_files) == 0:\n",
        "        print('No data files... quitting')\n",
        "        exit()\n",
        "\n",
        "    #return variables\n",
        "    sentences = []\n",
        "    word2idx = {'START':0, 'END':1}\n",
        "    idx2word = ['START', 'END']\n",
        "    current_idx = 2\n",
        "    word_idx_count = {0: float('inf'), 1: float('inf')}\n",
        "\n",
        "    if n_files is not None:\n",
        "        input_files = input_files[:n_files]\n",
        "\n",
        "    for f in input_files:\n",
        "        print(\"reading:\", f)\n",
        "        for line in open(wiki_data_location+'/' + f):\n",
        "            line = line.strip()\n",
        "            #ignore headers, structured data, lists etc...\n",
        "            if line and line[0] not in ('[', '*', '-', '|', '=', '{', '}'):\n",
        "                if by_paragraph:\n",
        "                    sentence_lines = [line]\n",
        "                else:\n",
        "                    sentence_lines = line.split('. ')\n",
        "\n",
        "                for sentence in sentence_lines:\n",
        "                    tokens = my_tokenizer(sentence)\n",
        "                    for t in tokens:\n",
        "                        if t not in word2idx:\n",
        "                            word2idx[t] = current_idx\n",
        "                            idx2word.append(t)\n",
        "                            current_idx += 1\n",
        "                        idx = word2idx[t]\n",
        "                        word_idx_count[idx] = word_idx_count.get(idx, 0) + 1    \n",
        "                    sentence_by_idx = [word2idx[t] for t in tokens]\n",
        "                    sentences.append(sentence_by_idx)\n",
        "\n",
        "    #restrict vocab size\n",
        "    sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    word2idx_small = {}\n",
        "    new_idx = 0\n",
        "    idx_new_idx_map = {}\n",
        "    for idx, count in sorted_word_idx_count[:n_vocab]:\n",
        "        word = idx2word[idx]\n",
        "        print(word, count)\n",
        "        word2idx_small[word] = new_idx\n",
        "        idx_new_idx_map[idx] = new_idx\n",
        "        new_idx += 1\n",
        "    #let 'unknown' be the last token\n",
        "    word2idx_small['UNKNOWN'] = new_idx\n",
        "    unknown = new_idx\n",
        "\n",
        "    #map old idx to new idx\n",
        "    sentences_small = []\n",
        "    for sentence in sentences:\n",
        "        if len(sentence) > 1:\n",
        "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
        "            sentences_small.append(new_sentence)\n",
        "\n",
        "    return sentences_small, word2idx_small\n",
        "\n",
        "def find_analogies(w1, w2, w3, We, word2idx, idx2word):\n",
        "    V, D = We.shape\n",
        "\n",
        "    king = We[word2idx[w1]]\n",
        "    man = We[word2idx[w2]]\n",
        "    woman = We[word2idx[w3]]\n",
        "    v0 = king - man + woman\n",
        "\n",
        "    for dist in ('euclidean', 'cosine'):\n",
        "        distances = pairwise_distances(v0.reshape(1, D), We, metric=dist).reshape(V)\n",
        "        idx = distances.argsort()[:4]\n",
        "        best_idx = -1\n",
        "        keep_out = [word2idx[w] for w in (w1, w2, w3)]\n",
        "        for i in idx:\n",
        "            if i not in keep_out:\n",
        "                best_idx = i\n",
        "                break\n",
        "        best_word = idx2word[best_idx]\n",
        "\n",
        "        print(\"Closest match by\", dist, \"distance:\", best_word)\n",
        "        print(w1, \"-\", w2, \"=\", best_word, \"-\", w3)\n",
        "\n",
        "def get_bigram_probs(sentences, V, start_idx, end_idx, smoothing=1):\n",
        "    # structure of bigram probability matrix will be:\n",
        "    # (last word, current word) --> probability\n",
        "    # we will use add-1 smoothing\n",
        "    # note: we will always ignore this from the END token\n",
        "    bigram_probs = np.ones((V,V)) * smoothing\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        for i in range(len(sentence)):\n",
        "            if i == 0:\n",
        "                #beginning word\n",
        "                bigram_probs[start_idx, sentence[i]] += 1\n",
        "            else:\n",
        "                #middle word\n",
        "                bigram_probs[sentence[i-1], sentence[i]] += 1\n",
        "\n",
        "            #if we are at the final word \n",
        "            #we update the bigram for last --> current \n",
        "            #and current --> END token\n",
        "            if i == len(sentence) - 1:\n",
        "                #final word\n",
        "                bigram_probs[sentence[i], end_idx] += 1\n",
        "\n",
        "    #normalize the counts along the rows to get probabilities\n",
        "    bigram_probs /= bigram_probs.sum(axis=1, keepdims=True)\n",
        "    return bigram_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvaDAkhFThFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}