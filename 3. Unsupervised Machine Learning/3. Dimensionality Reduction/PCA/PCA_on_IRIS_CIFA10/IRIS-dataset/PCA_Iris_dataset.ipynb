{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "You will find a five noisy versions of the Iris dataset, and a noiseless version. For each of the 5 noisy data sets, you should compute the principal components in two ways. In the first, you will use the mean and covariance matrix of the noiseless dataset. In the second, you will use the mean and covariance of the respective noisy datasets. Based on these components, you should compute the mean squared error between the noiseless version of the dataset and each of a PCA representation using 0 (i.e. every data item is represented by the mean), 1, 2, 3, and 4 principal components. The mean squared error here should compute the sum of the squared errors over the features and compute the mean of this over the rows. For example, if the noiseless version has two rows [1,2,3,4] and [0,0,0,0] and the reconstructed version is [1,2,3,0] and [1,1,1,1] the MSE would be (16 + 4) / 2 = 10\n",
    "\n",
    ". You should produce:\n",
    "A csv file showing your numbers filled in a table set out as below, where \"N\" columns represents the components calculated via the noiseless dataset and the \"c\" columns of the noisy datasets.\n",
    "Example: The entry corresponding to Dataset I and 2N should contain the mean squared error between the noiseless version of the dataset and the PCA representation of Dataset I, using 2 principal components computed from the mean and covariance matrix of the noiseless dataset.\n",
    "Update (for clarity of instructions): In all cases you compare the reconstruction with the noiseless dataset to get the MSE. \n",
    "The first part, with \"N\" columns asks to reconstruct the noisy datasets using the PCs of the noiseless dataset. \n",
    "The second part, with \"c\" columns asks to reconstruct the noisy datasets using the PCs of the noisy dataset.\n",
    "A csv file containing your reconstruction of Dataset I (\"dataI.csv\"), expanded onto 2 principal components, where mean and principal components are computed from Dataset I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_from_noiseless(datafile):\n",
    "    X_original = pd.read_csv('iris.csv').values\n",
    "    X_1 = pd.read_csv(datafile).values\n",
    "    MSE=[]\n",
    "    for c in range(0, 5):\n",
    "        clean_model = PCA(n_components=c)\n",
    "        clean_model.fit(X_original)\n",
    "        pca_data = clean_model.transform(X_1)\n",
    "        pca_data = clean_model.inverse_transform(pca_data)\n",
    "        result = (pca_data - X_original)**2\n",
    "        sum_of_columns = []\n",
    "        for i in range(len(result)):\n",
    "            sum_of_columns.append(result[i].sum())\n",
    "        sum_of_columns= np.array(sum_of_columns)\n",
    "        total_value = sum_of_columns.sum()/len(sum_of_columns)\n",
    "        MSE.append(total_value)\n",
    "    return  np.array(MSE)\n",
    "\n",
    "def calculate_from_noise(datafile):\n",
    "    X_original = pd.read_csv('iris.csv').values\n",
    "    X_1 = pd.read_csv(datafile).values\n",
    "    MSE=[]\n",
    "    for c in range(0, 5):\n",
    "        clean_model = PCA(n_components=c)\n",
    "        clean_model.fit(X_1)\n",
    "        pca_data = clean_model.transform(X_1)\n",
    "        pca_data = clean_model.inverse_transform(pca_data)\n",
    "        result = (pca_data - X_original)**2\n",
    "        sum_of_columns = []\n",
    "        for i in range(len(result)):\n",
    "            sum_of_columns.append(result[i].sum())\n",
    "        sum_of_columns= np.array(sum_of_columns)\n",
    "        total_value = sum_of_columns.sum()/len(sum_of_columns)\n",
    "        MSE.append(total_value)\n",
    "    return  np.array(MSE)\n",
    "\n",
    "#program starts here\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\sayan\\\\Documents\\\\UIUC\\\\Courses\\\\Semester-2\\\\CS498 AML\\\\hw3\\\\dataset')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import eigh\n",
    "import math\n",
    "\n",
    "files = ['dataI.csv', 'dataII.csv', 'dataIII.csv', 'dataIV.csv', 'dataV.csv']\n",
    "total = []\n",
    "for file in files:\n",
    "    x =  calculate_from_noiseless(file)\n",
    "    y =  calculate_from_noise(file)\n",
    "    temp = np.concatenate((x, y), axis=0)\n",
    "    total.append(list(temp))\n",
    "#output to part 1\n",
    "first_df = pd.DataFrame(data=total, columns = [\"0N\", \"1N\", \"2N\", \"3N\", \"4N\", \"0c\", \"1c\", \"2c\", \"3c\", \"4c\"])\n",
    "first_df.to_csv('sdutta26-numbers.csv',header=True, index=False)\n",
    "\n",
    "### part-2 ###\n",
    "\n",
    "X_1 = pd.read_csv('dataI.csv').values\n",
    "clean_model = PCA(n_components=2)\n",
    "clean_model.fit(X_1)\n",
    "pca_data = clean_model.transform(X_1)\n",
    "pca_data = clean_model.inverse_transform(pca_data)\n",
    "output_df = pd.DataFrame(data=pca_data,columns = [\"Sepal.Length\",\"Sepal.Width\",\"Petal.Length\",\"Petal.Width\"])\n",
    "output_df.to_csv('sdutta26-recon.csv',index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
